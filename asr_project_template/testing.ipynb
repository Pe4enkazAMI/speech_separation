{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 47999])\n"
     ]
    }
   ],
   "source": [
    "from hw_ss.model.SpEX_plus import SpEXPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = {\n",
    "    \"L1\": 20,\n",
    "      \"L2\": 80,\n",
    "      \"L3\": 160,\n",
    "      \"speech_encoder_out_channels\": 30,\n",
    "      \"extractor_emb_dim\": 20,\n",
    "      \"extractor_intermed_dim\": 10,\n",
    "      \"num_tcn_blocks_in_stack\": 2,\n",
    "      \"num_stacked_tcn\": 2,\n",
    "      \"num_res_net_blocks\": 2,\n",
    "      \"spk_emb\": 100,\n",
    "      \"num_spk\": 2,\n",
    "      \"tcn_kernel_size\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpEXPlus(**kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpEXPlus(\n",
       "  (speech_and_speaker_encoder): SpeechEncoder(\n",
       "    (encoder_short): Conv1d(1, 30, kernel_size=(20,), stride=(10,))\n",
       "    (encoder_middle): Conv1d(1, 30, kernel_size=(80,), stride=(10,))\n",
       "    (encoder_long): Conv1d(1, 30, kernel_size=(160,), stride=(10,))\n",
       "  )\n",
       "  (channel_norm): ChannelWiseLayerNorm(\n",
       "    (norm): LayerNorm((90,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (conv1x1_extractor): Conv1d(90, 20, kernel_size=(1,), stride=(1,))\n",
       "  (tcn_extractors): SpeakerExtractor(\n",
       "    (extractor): ModuleDict(\n",
       "      (StackedTCN #1): StackedTCN(\n",
       "        (STCN): ModuleDict(\n",
       "          (FTCN): FTCNBlock(\n",
       "            (block): Sequential(\n",
       "              (0): Conv1d(120, 10, kernel_size=(1,), stride=(1,))\n",
       "              (1): PReLU(num_parameters=1)\n",
       "              (2): GlobalLayerNorm()\n",
       "              (3): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(1,), groups=10)\n",
       "              (4): PReLU(num_parameters=1)\n",
       "              (5): GlobalLayerNorm()\n",
       "              (6): Conv1d(10, 20, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (TCN #1): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(20, 10, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=10)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(10, 20, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (StackedTCN #2): StackedTCN(\n",
       "        (STCN): ModuleDict(\n",
       "          (FTCN): FTCNBlock(\n",
       "            (block): Sequential(\n",
       "              (0): Conv1d(120, 10, kernel_size=(1,), stride=(1,))\n",
       "              (1): PReLU(num_parameters=1)\n",
       "              (2): GlobalLayerNorm()\n",
       "              (3): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(1,), groups=10)\n",
       "              (4): PReLU(num_parameters=1)\n",
       "              (5): GlobalLayerNorm()\n",
       "              (6): Conv1d(10, 20, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (TCN #1): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(20, 10, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=10)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(10, 20, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mask_short): Conv1d(20, 30, kernel_size=(1,), stride=(1,))\n",
       "  (mask_middle): Conv1d(20, 30, kernel_size=(1,), stride=(1,))\n",
       "  (mask_long): Conv1d(20, 30, kernel_size=(1,), stride=(1,))\n",
       "  (speech_decoder): SpeechDecoder(\n",
       "    (decoder_short): ConvTranspose1d(30, 1, kernel_size=(20,), stride=(10,))\n",
       "    (decoder_middle): ConvTranspose1d(30, 1, kernel_size=(80,), stride=(10,))\n",
       "    (decoder_long): ConvTranspose1d(30, 1, kernel_size=(160,), stride=(10,))\n",
       "  )\n",
       "  (channel_norm_speaker): ChannelWiseLayerNorm(\n",
       "    (norm): LayerNorm((90,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (conv1x1_speaker): Conv1d(90, 20, kernel_size=(1,), stride=(1,))\n",
       "  (speaker_encoder): SpeakerEncoder(\n",
       "    (spk_enc): Sequential(\n",
       "      (0): ResBlock(\n",
       "        (fblock): Sequential(\n",
       "          (0): Conv1d(20, 10, kernel_size=(1,), stride=(1,))\n",
       "          (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv1d(10, 10, kernel_size=(1,), stride=(1,))\n",
       "          (4): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (down): Conv1d(20, 10, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (sblock): Sequential(\n",
       "          (0): PReLU(num_parameters=1)\n",
       "          (1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (fblock): Sequential(\n",
       "          (0): Conv1d(10, 10, kernel_size=(1,), stride=(1,))\n",
       "          (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv1d(10, 10, kernel_size=(1,), stride=(1,))\n",
       "          (4): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (sblock): Sequential(\n",
       "          (0): PReLU(num_parameters=1)\n",
       "          (1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Conv1d(10, 100, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (speaker_logits): SpeakerClassificationHead(\n",
       "    (linear): Linear(in_features=100, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "batch = torch.randn(10, 1, 48000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.audio.pesq import PerceptualEvaluationSpeechQuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesq = PerceptualEvaluationSpeechQuality(16000, \"nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3714)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesq(batch, torch.ones_like(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1, s2, s3, pred = model(batch, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 48000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SISDRLoss(pred, target):\n",
    "    pred = pred.squeeze(1)\n",
    "    target = target.squeeze(1)\n",
    "\n",
    "    pred_hat = pred - pred.mean(dim=-1).unsqueeze(-1)\n",
    "    target_hat = target - target.mean(dim=-1).unsqueeze(-1)\n",
    "\n",
    "    pred_t = torch.inner(pred_hat, target_hat) * target_hat / (target_hat ** 2).sum(dim=-1)\n",
    "    target_t = pred_hat - pred_t\n",
    "    \n",
    "    ret = 20 * torch.log10(torch.norm(pred_t) / (torch.norm(target_t) + 1e-6))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'unsqeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/bayesian_monster/speech_separation/asr_project_template/testing.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bayesian_monster/speech_separation/asr_project_template/testing.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m SISDRLoss(torch\u001b[39m.\u001b[39;49mrandn(size\u001b[39m=\u001b[39;49m(\u001b[39m3\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m10\u001b[39;49m)), torch\u001b[39m.\u001b[39;49mrandn(size\u001b[39m=\u001b[39;49m(\u001b[39m3\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m10\u001b[39;49m)))\n",
      "\u001b[1;32m/Users/bayesian_monster/speech_separation/asr_project_template/testing.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bayesian_monster/speech_separation/asr_project_template/testing.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bayesian_monster/speech_separation/asr_project_template/testing.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bayesian_monster/speech_separation/asr_project_template/testing.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pred_hat \u001b[39m=\u001b[39m pred \u001b[39m-\u001b[39m pred\u001b[39m.\u001b[39;49mmean(dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49munsqeeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bayesian_monster/speech_separation/asr_project_template/testing.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m target_hat \u001b[39m=\u001b[39m target \u001b[39m-\u001b[39m target\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bayesian_monster/speech_separation/asr_project_template/testing.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pred_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39minner(pred_hat, target_hat) \u001b[39m*\u001b[39m target_hat \u001b[39m/\u001b[39m (target_hat \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'unsqeeze'"
     ]
    }
   ],
   "source": [
    "SISDRLoss(torch.randn(size=(3, 1, 10)), torch.randn(size=(3, 1, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(size=(3, 10))\n",
    "b = torch.randn(size=(3, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(b - b.mean(dim=-1).unsqueeze(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = linear_proj(subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feed_forward import ConformerFeedForwardLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffl = ConformerFeedForwardLayer(512, 2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_ffl = ffl(lin) + lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ConformerAttentionBlock(512, 8, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_attn = attn(after_ffl) + after_ffl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conv import ConformerConvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ = ConformerConvBlock(in_chanels=512, exp_factor=2, kernel_size=31, padding=15, stride=1, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 31, 512])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1024, 31])\n",
      "torch.Size([3, 512, 31])\n",
      "torch.Size([3, 512, 31])\n",
      "torch.Size([3, 512, 31])\n",
      "torch.Size([3, 512, 31])\n",
      "torch.Size([3, 512, 31])\n",
      "torch.Size([3, 512, 31])\n"
     ]
    }
   ],
   "source": [
    "after_conv = conv_(after_attn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffl_2 = ConformerFeedForwardLayer(512, 2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "shit = (ffl_2(after_conv) + after_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 31, 512])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvSubsample(nn.Module):\n",
    "    def __init__(self, in_chanels, out_chanels):\n",
    "        super().__init__()\n",
    "        self.subsampler = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=out_chanels, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=out_chanels, out_channels=out_chanels, kernel_size=3, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.subsampler(x.unsqueeze(1))\n",
    "        batch_size, channels, subsample_len, subsample_dim = out.shape\n",
    "\n",
    "        out = out.permute(0, 2, 1, 3)\n",
    "        out = out.contiguous().view(batch_size, subsample_len, channels * subsample_dim)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_jopa = ConvSubsample(1, 144)\n",
    "linear = nn.Linear(144 * ((140 - 1)//2 - 1)//2, 144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 140])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.randn(3, 128, 140)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 144 31 34\n",
      "4896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 31, 144])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = testing_jopa(batch)\n",
    "linear(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4896"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "144 * (((140 - 1) // 2 - 1) // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1027"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('/kaggle/working/MixTrain360/1027_6505_000118_0-ref.wav'.split(\"/\")[-1].split(\"_\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 jopa\n",
      "1 koka\n"
     ]
    }
   ],
   "source": [
    "a = [\"jopa\", \"koka\"]\n",
    "for k,v in enumerate(a):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
