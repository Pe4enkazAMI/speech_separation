{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 47999])\n"
     ]
    }
   ],
   "source": [
    "from hw_ss.model.SpEX_plus import SpEXPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = {\n",
    "    \"L1\": 20,\n",
    "      \"L2\": 80,\n",
    "      \"L3\": 160,\n",
    "      \"speech_encoder_out_channels\": 256,\n",
    "      \"extractor_emb_dim\": 256,\n",
    "      \"extractor_intermed_dim\": 512,\n",
    "      \"num_tcn_blocks_in_stack\": 8,\n",
    "      \"num_stacked_tcn\": 4,\n",
    "      \"num_res_net_blocks\": 3,\n",
    "      \"spk_emb\": 256,\n",
    "      \"num_spk\": 251,\n",
    "      \"tcn_kernel_size\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpEXPlus(**kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpEXPlus(\n",
       "  (speech_and_speaker_encoder): SpeechEncoder(\n",
       "    (encoder_short): Conv1d(1, 30, kernel_size=(20,), stride=(10,))\n",
       "    (encoder_middle): Conv1d(1, 30, kernel_size=(80,), stride=(10,))\n",
       "    (encoder_long): Conv1d(1, 30, kernel_size=(160,), stride=(10,))\n",
       "  )\n",
       "  (channel_norm): ChannelWiseLayerNorm(\n",
       "    (norm): LayerNorm((90,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (conv1x1_extractor): Conv1d(90, 20, kernel_size=(1,), stride=(1,))\n",
       "  (tcn_extractors): SpeakerExtractor(\n",
       "    (extractor): ModuleDict(\n",
       "      (StackedTCN #1): StackedTCN(\n",
       "        (STCN): ModuleDict(\n",
       "          (FTCN): FTCNBlock(\n",
       "            (block): Sequential(\n",
       "              (0): Conv1d(120, 10, kernel_size=(1,), stride=(1,))\n",
       "              (1): PReLU(num_parameters=1)\n",
       "              (2): GlobalLayerNorm()\n",
       "              (3): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(1,), groups=10)\n",
       "              (4): PReLU(num_parameters=1)\n",
       "              (5): GlobalLayerNorm()\n",
       "              (6): Conv1d(10, 20, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (TCN #1): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(20, 10, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=10)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(10, 20, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (StackedTCN #2): StackedTCN(\n",
       "        (STCN): ModuleDict(\n",
       "          (FTCN): FTCNBlock(\n",
       "            (block): Sequential(\n",
       "              (0): Conv1d(120, 10, kernel_size=(1,), stride=(1,))\n",
       "              (1): PReLU(num_parameters=1)\n",
       "              (2): GlobalLayerNorm()\n",
       "              (3): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(1,), groups=10)\n",
       "              (4): PReLU(num_parameters=1)\n",
       "              (5): GlobalLayerNorm()\n",
       "              (6): Conv1d(10, 20, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (TCN #1): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(20, 10, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=10)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(10, 20, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mask_short): Conv1d(20, 30, kernel_size=(1,), stride=(1,))\n",
       "  (mask_middle): Conv1d(20, 30, kernel_size=(1,), stride=(1,))\n",
       "  (mask_long): Conv1d(20, 30, kernel_size=(1,), stride=(1,))\n",
       "  (speech_decoder): SpeechDecoder(\n",
       "    (decoder_short): ConvTranspose1d(30, 1, kernel_size=(20,), stride=(10,))\n",
       "    (decoder_middle): ConvTranspose1d(30, 1, kernel_size=(80,), stride=(10,))\n",
       "    (decoder_long): ConvTranspose1d(30, 1, kernel_size=(160,), stride=(10,))\n",
       "  )\n",
       "  (channel_norm_speaker): ChannelWiseLayerNorm(\n",
       "    (norm): LayerNorm((90,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (conv1x1_speaker): Conv1d(90, 20, kernel_size=(1,), stride=(1,))\n",
       "  (speaker_encoder): SpeakerEncoder(\n",
       "    (spk_enc): Sequential(\n",
       "      (0): ResBlock(\n",
       "        (fblock): Sequential(\n",
       "          (0): Conv1d(20, 10, kernel_size=(1,), stride=(1,))\n",
       "          (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv1d(10, 10, kernel_size=(1,), stride=(1,))\n",
       "          (4): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (down): Conv1d(20, 10, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (sblock): Sequential(\n",
       "          (0): PReLU(num_parameters=1)\n",
       "          (1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (fblock): Sequential(\n",
       "          (0): Conv1d(10, 10, kernel_size=(1,), stride=(1,))\n",
       "          (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv1d(10, 10, kernel_size=(1,), stride=(1,))\n",
       "          (4): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (sblock): Sequential(\n",
       "          (0): PReLU(num_parameters=1)\n",
       "          (1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Conv1d(10, 100, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (speaker_logits): SpeakerClassificationHead(\n",
       "    (linear): Linear(in_features=100, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "batch = torch.randn(10, 1, 48000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.audio.pesq import PerceptualEvaluationSpeechQuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesq = PerceptualEvaluationSpeechQuality(16000, \"nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3714)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesq(batch, torch.ones_like(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1, s2, s3, pred = model(batch, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 48000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(pred, target):\n",
    "    eps = torch.finfo(pred.dtype).eps\n",
    "    pred = pred.squeeze(1)\n",
    "    target = target.squeeze(1)\n",
    "    assert pred.shape == target.shape, \"pred shapes should be the same as target shapes\"\n",
    "\n",
    "    target = target - torch.mean(target, dim=-1, keepdim=True)\n",
    "    pred = pred - torch.mean(pred, dim=-1, keepdim=True)\n",
    "    alpha = (torch.sum(pred * target, dim=-1, keepdim=True) + eps)/(torch.sum(target**2, dim=-1, keepdim=True) + eps)\n",
    "    target_scaled = alpha * target\n",
    "    noise = target_scaled - pred \n",
    "\n",
    "    val = (torch.sum(target_scaled**2, dim=-1) + eps) / (torch.sum(noise**2, dim=-1) + eps)\n",
    "    val = 10 * torch.log10(val)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute(torch.randn(size=(100, 1, 10)), torch.randn(size=(100, 1, 10))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(size=(3, 10))\n",
    "b = torch.randn(size=(3, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.8380, -0.3416,  3.9803],\n",
       "        [-2.4620, -2.9715, -5.4651],\n",
       "        [ 2.9693, -3.7212, -1.3142]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.inner(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = linear_proj(subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feed_forward import ConformerFeedForwardLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffl = ConformerFeedForwardLayer(512, 2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_ffl = ffl(lin) + lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ConformerAttentionBlock(512, 8, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_attn = attn(after_ffl) + after_ffl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conv import ConformerConvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ = ConformerConvBlock(in_chanels=512, exp_factor=2, kernel_size=31, padding=15, stride=1, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 31, 512])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1024, 31])\n",
      "torch.Size([3, 512, 31])\n",
      "torch.Size([3, 512, 31])\n",
      "torch.Size([3, 512, 31])\n",
      "torch.Size([3, 512, 31])\n",
      "torch.Size([3, 512, 31])\n",
      "torch.Size([3, 512, 31])\n"
     ]
    }
   ],
   "source": [
    "after_conv = conv_(after_attn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffl_2 = ConformerFeedForwardLayer(512, 2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "shit = (ffl_2(after_conv) + after_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 31, 512])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvSubsample(nn.Module):\n",
    "    def __init__(self, in_chanels, out_chanels):\n",
    "        super().__init__()\n",
    "        self.subsampler = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=out_chanels, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=out_chanels, out_channels=out_chanels, kernel_size=3, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.subsampler(x.unsqueeze(1))\n",
    "        batch_size, channels, subsample_len, subsample_dim = out.shape\n",
    "\n",
    "        out = out.permute(0, 2, 1, 3)\n",
    "        out = out.contiguous().view(batch_size, subsample_len, channels * subsample_dim)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_jopa = ConvSubsample(1, 144)\n",
    "linear = nn.Linear(144 * ((140 - 1)//2 - 1)//2, 144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 140])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.randn(3, 128, 140)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 144 31 34\n",
      "4896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 31, 144])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = testing_jopa(batch)\n",
    "linear(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4896"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "144 * (((140 - 1) // 2 - 1) // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1027"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('/kaggle/working/MixTrain360/1027_6505_000118_0-ref.wav'.split(\"/\")[-1].split(\"_\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 jopa\n",
      "1 koka\n"
     ]
    }
   ],
   "source": [
    "a = [\"jopa\", \"koka\"]\n",
    "for k,v in enumerate(a):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
