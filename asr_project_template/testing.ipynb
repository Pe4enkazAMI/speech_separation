{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw_ss.model.SpEX_plus import SpEXPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = {\n",
    "    \"L1\": 20,\n",
    "    \"L2\": 80,\n",
    "    \"L3\": 160,\n",
    "    \"speech_encoder_out_channels\": 256,\n",
    "    \"num_tcn_blocks_in_stack\": 8,\n",
    "    \"num_stacked_tcn\": 2,\n",
    "    \"num_res_net_blocks\": 2,\n",
    "    \"spk_emb\": 256,\n",
    "    \"num_spk\": 101,\n",
    "    \"extractor_emb_dim\": 256,\n",
    "    \"extractor_intermed_dim\":512,\n",
    "    \"tcn_kernel_size\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpEXPlus(**kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpEXPlus(\n",
       "  (speech_and_speaker_encoder): SpeechEncoder(\n",
       "    (encoder_short): Conv1d(1, 256, kernel_size=(20,), stride=(10,))\n",
       "    (encoder_middle): Conv1d(1, 256, kernel_size=(80,), stride=(10,))\n",
       "    (encoder_long): Conv1d(1, 256, kernel_size=(160,), stride=(10,))\n",
       "  )\n",
       "  (channel_norm): ChannelWiseLayerNorm(\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (conv1x1_extractor): Conv1d(768, 256, kernel_size=(1,), stride=(1,))\n",
       "  (tcn_extractors): SpeakerExtractor(\n",
       "    (extractor): ModuleDict(\n",
       "      (StackedTCN #1): StackedTCN(\n",
       "        (STCN): ModuleDict(\n",
       "          (FTCN): FTCNBlock(\n",
       "            (block): Sequential(\n",
       "              (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "              (1): PReLU(num_parameters=1)\n",
       "              (2): GlobalLayerNorm()\n",
       "              (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512)\n",
       "              (4): PReLU(num_parameters=1)\n",
       "              (5): GlobalLayerNorm()\n",
       "              (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (TCN #1): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (TCN #2): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (TCN #3): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (TCN #4): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (TCN #5): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (TCN #6): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (TCN #7): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (StackedTCN #2): StackedTCN(\n",
       "        (STCN): ModuleDict(\n",
       "          (FTCN): FTCNBlock(\n",
       "            (block): Sequential(\n",
       "              (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "              (1): PReLU(num_parameters=1)\n",
       "              (2): GlobalLayerNorm()\n",
       "              (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512)\n",
       "              (4): PReLU(num_parameters=1)\n",
       "              (5): GlobalLayerNorm()\n",
       "              (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (TCN #1): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (TCN #2): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (TCN #3): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (TCN #4): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (TCN #5): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (TCN #6): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (TCN #7): TCNblock(\n",
       "            (block): ResidualConnection(\n",
       "              (module): Sequential(\n",
       "                (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                (1): PReLU(num_parameters=1)\n",
       "                (2): GlobalLayerNorm()\n",
       "                (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512)\n",
       "                (4): PReLU(num_parameters=1)\n",
       "                (5): GlobalLayerNorm()\n",
       "                (6): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mask_short): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "  (mask_middle): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "  (mask_long): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "  (speech_decoder): SpeechDecoder(\n",
       "    (decoder_short): ConvTranspose1d(256, 1, kernel_size=(20,), stride=(10,))\n",
       "    (decoder_middle): ConvTranspose1d(256, 1, kernel_size=(80,), stride=(10,))\n",
       "    (decoder_long): ConvTranspose1d(256, 1, kernel_size=(160,), stride=(10,))\n",
       "  )\n",
       "  (channel_norm_speaker): ChannelWiseLayerNorm(\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (conv1x1_speaker): Conv1d(768, 256, kernel_size=(1,), stride=(1,))\n",
       "  (speaker_encoder): SpeakerEncoder(\n",
       "    (spk_enc): Sequential(\n",
       "      (0): ResBlock(\n",
       "        (fblock): Sequential(\n",
       "          (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (down): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (sblock): Sequential(\n",
       "          (0): PReLU(num_parameters=1)\n",
       "          (1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (fblock): Sequential(\n",
       "          (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): PReLU(num_parameters=1)\n",
       "          (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (sblock): Sequential(\n",
       "          (0): PReLU(num_parameters=1)\n",
       "          (1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (speaker_logits): SpeakerClassificationHead(\n",
       "    (linear): Linear(in_features=256, out_features=101, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 100])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.randn(3, 1, 100)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER SPEECH AND SPEAKER: tensor([[[0.0000, 0.4008, 0.0000,  ..., 0.0000, 0.0775, 0.0000],\n",
      "         [0.1084, 1.3018, 0.5662,  ..., 0.0000, 0.0000, 0.4259],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.5513, 1.4991, 0.7461],\n",
      "         ...,\n",
      "         [0.8323, 0.0000, 0.0000,  ..., 0.1682, 1.1922, 0.5257],\n",
      "         [0.2505, 0.7167, 0.4086,  ..., 0.0000, 0.0251, 0.1525],\n",
      "         [1.0955, 0.0000, 0.3847,  ..., 1.2100, 0.8983, 0.0000]],\n",
      "\n",
      "        [[0.6957, 0.0000, 0.0000,  ..., 0.4294, 0.9368, 0.0000],\n",
      "         [0.2091, 0.1107, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.4659, 0.0000, 0.5078,  ..., 1.4263, 0.0000, 0.5699],\n",
      "         ...,\n",
      "         [0.6354, 0.3962, 0.0000,  ..., 0.4655, 0.0000, 1.5718],\n",
      "         [0.5411, 0.0000, 0.0000,  ..., 0.0212, 0.0000, 1.0932],\n",
      "         [0.0000, 0.9326, 0.0000,  ..., 0.0000, 0.0000, 1.0216]],\n",
      "\n",
      "        [[0.0000, 0.2618, 0.3787,  ..., 0.4978, 0.1400, 0.0000],\n",
      "         [0.3629, 0.0000, 0.0000,  ..., 0.5358, 0.0000, 0.0000],\n",
      "         [1.0708, 0.1833, 0.5505,  ..., 0.6682, 0.5056, 0.3625],\n",
      "         ...,\n",
      "         [0.2681, 0.4218, 0.2648,  ..., 0.0000, 1.2643, 0.2146],\n",
      "         [0.3597, 0.0000, 0.2222,  ..., 0.2418, 0.4312, 0.2035],\n",
      "         [0.0000, 0.4644, 0.3141,  ..., 0.2312, 0.4143, 0.2550]]],\n",
      "       grad_fn=<ReluBackward0>) tensor([[[0.4689, 0.0000, 0.0000,  ..., 0.0626, 0.6187, 0.0000],\n",
      "         [0.0000, 0.2912, 0.0000,  ..., 0.0000, 0.0000, 0.0047],\n",
      "         [0.2753, 0.0000, 0.0000,  ..., 0.0000, 0.1220, 0.4287],\n",
      "         ...,\n",
      "         [0.0000, 0.8033, 0.0000,  ..., 0.4588, 0.1359, 0.0000],\n",
      "         [0.3951, 0.3929, 0.0000,  ..., 0.0000, 0.3636, 0.4294],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3442, 0.0212, 0.0343,  ..., 0.0000, 0.1012, 0.5364],\n",
      "         [0.0000, 0.0392, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.9686, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.5134],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2363, 0.3527, 0.3448],\n",
      "         [0.1106, 0.1807, 0.0000,  ..., 0.8574, 0.0000, 0.3100],\n",
      "         [0.1611, 0.0000, 1.1590,  ..., 0.5610, 0.2131, 0.2409]],\n",
      "\n",
      "        [[0.0164, 0.6478, 0.3469,  ..., 0.3280, 0.1133, 0.2276],\n",
      "         [0.0000, 0.7689, 0.1139,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6695, 0.3845],\n",
      "         ...,\n",
      "         [0.0000, 0.4845, 0.0000,  ..., 0.0588, 0.5964, 0.0000],\n",
      "         [0.3534, 0.7872, 0.1681,  ..., 0.0000, 0.4452, 0.0000],\n",
      "         [0.0000, 0.3724, 0.4074,  ..., 0.0000, 0.2290, 0.0000]]],\n",
      "       grad_fn=<ReluBackward0>) tensor([[[0.0000, 0.0000, 0.3167,  ..., 0.2335, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.5303,  ..., 0.4410, 0.1580, 0.0000],\n",
      "         [0.0000, 0.0000, 0.2491,  ..., 0.2147, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.5411, 0.3768, 0.0000,  ..., 0.0000, 0.0000, 0.4690],\n",
      "         [0.0000, 0.2502, 0.4590,  ..., 0.5801, 0.2157, 0.0986]],\n",
      "\n",
      "        [[0.0000, 0.1688, 0.5608,  ..., 0.0000, 0.3261, 0.0000],\n",
      "         [0.7231, 0.0000, 1.1937,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.4515, 0.0321, 0.4896,  ..., 0.2816, 0.0000, 0.1381],\n",
      "         ...,\n",
      "         [0.1693, 0.0000, 0.0539,  ..., 0.0000, 0.1656, 0.0000],\n",
      "         [0.0000, 0.9163, 0.0000,  ..., 0.5214, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.6569,  ..., 0.0000, 0.6688, 0.0430]],\n",
      "\n",
      "        [[0.0000, 0.3714, 0.1812,  ..., 0.2043, 0.0000, 0.0029],\n",
      "         [0.0000, 0.6100, 0.2240,  ..., 0.2534, 0.0000, 0.2684],\n",
      "         [0.0859, 0.0000, 0.4089,  ..., 0.3956, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0124],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0000],\n",
      "         [0.0000, 1.1501, 0.5173,  ..., 0.6002, 0.0000, 0.0000]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "AFTER CHANNEL NORM: tensor([[[-0.6577,  0.7007, -0.6895,  ..., -0.5958, -0.3284, -0.6076],\n",
      "         [-0.3186,  3.7963,  1.2480,  ..., -0.5958, -0.6345,  1.1988],\n",
      "         [-0.6577, -0.6762, -0.6895,  ...,  1.5816,  5.2845,  2.5574],\n",
      "         ...,\n",
      "         [-0.6577, -0.6762, -0.6895,  ..., -0.5958, -0.6345, -0.6076],\n",
      "         [ 1.0351,  0.6182, -0.6895,  ..., -0.5958, -0.6345,  1.3817],\n",
      "         [-0.6577,  0.1833,  0.8812,  ...,  1.6955,  0.2170, -0.1892]],\n",
      "\n",
      "        [[ 1.3787, -0.6515, -0.6576,  ...,  0.7104,  2.2550, -0.6001],\n",
      "         [-0.0877, -0.2936, -0.6576,  ..., -0.6423, -0.6372, -0.6001],\n",
      "         [ 0.6864, -0.6515,  0.7898,  ...,  3.8509, -0.6372,  1.4571],\n",
      "         ...,\n",
      "         [-0.2074, -0.6515, -0.5040,  ..., -0.6423, -0.1259, -0.6001],\n",
      "         [-0.7177,  2.3094, -0.6576,  ...,  1.0001, -0.6372, -0.6001],\n",
      "         [-0.7177, -0.6515,  1.2149,  ..., -0.6423,  1.4277, -0.4450]],\n",
      "\n",
      "        [[-0.6678,  0.2566,  0.6228,  ...,  1.1755, -0.0388, -0.5851],\n",
      "         [ 0.6579, -0.6737, -0.6630,  ...,  1.3140, -0.6594, -0.5851],\n",
      "         [ 3.2438, -0.0222,  1.2064,  ...,  1.7972,  1.5820,  1.0831],\n",
      "         ...,\n",
      "         [-0.6678, -0.6737, -0.6630,  ..., -0.6407, -0.6594, -0.5282],\n",
      "         [-0.6678, -0.6737, -0.6630,  ..., -0.6326, -0.6594, -0.5851],\n",
      "         [-0.6678,  3.4135,  1.0935,  ...,  1.5490, -0.6594, -0.5851]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "AFTER 1x1 CONV: tensor([[[ 0.4351,  0.4056, -0.4384,  ..., -0.0849,  0.4574, -0.1672],\n",
      "         [-0.1627, -1.0955, -0.9031,  ...,  0.8085, -0.7070, -1.3725],\n",
      "         [-0.9790,  0.3053,  1.1656,  ..., -0.0814, -0.2009,  0.9234],\n",
      "         ...,\n",
      "         [ 0.0244,  0.0589,  0.3651,  ...,  0.4389,  0.2501,  0.4696],\n",
      "         [-0.2607,  0.5882,  0.3153,  ...,  0.1543, -0.3240, -0.0468],\n",
      "         [-0.6231, -0.3035,  0.4668,  ...,  0.0400, -0.1007,  0.0691]],\n",
      "\n",
      "        [[ 0.4833, -0.5398,  0.6108,  ...,  0.2801,  0.5216,  0.2741],\n",
      "         [ 0.2737,  0.5722,  0.2101,  ..., -0.5953, -0.4600,  1.0073],\n",
      "         [-0.8186, -0.4938, -0.3548,  ..., -0.0174,  0.3448, -0.4449],\n",
      "         ...,\n",
      "         [ 0.1026,  0.7125, -0.8250,  ..., -0.2851,  0.9217,  0.6666],\n",
      "         [ 0.8251,  0.2899,  0.6253,  ..., -0.3655, -0.5720, -0.9903],\n",
      "         [ 0.0594,  0.6962, -0.3262,  ..., -0.5938,  0.2881, -1.0090]],\n",
      "\n",
      "        [[ 0.1262,  0.8647,  0.5839,  ...,  0.2335,  0.4009,  0.3268],\n",
      "         [ 1.5213, -1.0436,  0.4601,  ...,  0.1418,  1.4603,  0.1574],\n",
      "         [-0.8601, -0.3982,  1.1079,  ...,  0.9334, -0.4170,  0.5959],\n",
      "         ...,\n",
      "         [-0.0700,  0.0567, -0.1833,  ...,  0.3745,  0.8051, -0.1313],\n",
      "         [-0.3952, -0.6346, -0.3181,  ...,  0.2041, -0.0239, -0.2916],\n",
      "         [ 0.5098,  0.2612, -1.1918,  ...,  0.0847, -0.6157,  0.7194]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "AFTER SPEECH AND SPEAKER (SPEAKER): tensor([[[0.0000, 0.4008, 0.0000,  ..., 0.0000, 0.0775, 0.0000],\n",
      "         [0.1084, 1.3018, 0.5662,  ..., 0.0000, 0.0000, 0.4259],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.5513, 1.4991, 0.7461],\n",
      "         ...,\n",
      "         [0.8323, 0.0000, 0.0000,  ..., 0.1682, 1.1922, 0.5257],\n",
      "         [0.2505, 0.7167, 0.4086,  ..., 0.0000, 0.0251, 0.1525],\n",
      "         [1.0955, 0.0000, 0.3847,  ..., 1.2100, 0.8983, 0.0000]],\n",
      "\n",
      "        [[0.6957, 0.0000, 0.0000,  ..., 0.4294, 0.9368, 0.0000],\n",
      "         [0.2091, 0.1107, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.4659, 0.0000, 0.5078,  ..., 1.4263, 0.0000, 0.5699],\n",
      "         ...,\n",
      "         [0.6354, 0.3962, 0.0000,  ..., 0.4655, 0.0000, 1.5718],\n",
      "         [0.5411, 0.0000, 0.0000,  ..., 0.0212, 0.0000, 1.0932],\n",
      "         [0.0000, 0.9326, 0.0000,  ..., 0.0000, 0.0000, 1.0216]],\n",
      "\n",
      "        [[0.0000, 0.2618, 0.3787,  ..., 0.4978, 0.1400, 0.0000],\n",
      "         [0.3629, 0.0000, 0.0000,  ..., 0.5358, 0.0000, 0.0000],\n",
      "         [1.0708, 0.1833, 0.5505,  ..., 0.6682, 0.5056, 0.3625],\n",
      "         ...,\n",
      "         [0.2681, 0.4218, 0.2648,  ..., 0.0000, 1.2643, 0.2146],\n",
      "         [0.3597, 0.0000, 0.2222,  ..., 0.2418, 0.4312, 0.2035],\n",
      "         [0.0000, 0.4644, 0.3141,  ..., 0.2312, 0.4143, 0.2550]]],\n",
      "       grad_fn=<ReluBackward0>) tensor([[[0.4689, 0.0000, 0.0000,  ..., 0.0626, 0.6187, 0.0000],\n",
      "         [0.0000, 0.2912, 0.0000,  ..., 0.0000, 0.0000, 0.0047],\n",
      "         [0.2753, 0.0000, 0.0000,  ..., 0.0000, 0.1220, 0.4287],\n",
      "         ...,\n",
      "         [0.0000, 0.8033, 0.0000,  ..., 0.4588, 0.1359, 0.0000],\n",
      "         [0.3951, 0.3929, 0.0000,  ..., 0.0000, 0.3636, 0.4294],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3442, 0.0212, 0.0343,  ..., 0.0000, 0.1012, 0.5364],\n",
      "         [0.0000, 0.0392, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.9686, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.5134],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.2363, 0.3527, 0.3448],\n",
      "         [0.1106, 0.1807, 0.0000,  ..., 0.8574, 0.0000, 0.3100],\n",
      "         [0.1611, 0.0000, 1.1590,  ..., 0.5610, 0.2131, 0.2409]],\n",
      "\n",
      "        [[0.0164, 0.6478, 0.3469,  ..., 0.3280, 0.1133, 0.2276],\n",
      "         [0.0000, 0.7689, 0.1139,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6695, 0.3845],\n",
      "         ...,\n",
      "         [0.0000, 0.4845, 0.0000,  ..., 0.0588, 0.5964, 0.0000],\n",
      "         [0.3534, 0.7872, 0.1681,  ..., 0.0000, 0.4452, 0.0000],\n",
      "         [0.0000, 0.3724, 0.4074,  ..., 0.0000, 0.2290, 0.0000]]],\n",
      "       grad_fn=<ReluBackward0>) tensor([[[0.0000, 0.0000, 0.3167,  ..., 0.2335, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.5303,  ..., 0.4410, 0.1580, 0.0000],\n",
      "         [0.0000, 0.0000, 0.2491,  ..., 0.2147, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.5411, 0.3768, 0.0000,  ..., 0.0000, 0.0000, 0.4690],\n",
      "         [0.0000, 0.2502, 0.4590,  ..., 0.5801, 0.2157, 0.0986]],\n",
      "\n",
      "        [[0.0000, 0.1688, 0.5608,  ..., 0.0000, 0.3261, 0.0000],\n",
      "         [0.7231, 0.0000, 1.1937,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.4515, 0.0321, 0.4896,  ..., 0.2816, 0.0000, 0.1381],\n",
      "         ...,\n",
      "         [0.1693, 0.0000, 0.0539,  ..., 0.0000, 0.1656, 0.0000],\n",
      "         [0.0000, 0.9163, 0.0000,  ..., 0.5214, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.6569,  ..., 0.0000, 0.6688, 0.0430]],\n",
      "\n",
      "        [[0.0000, 0.3714, 0.1812,  ..., 0.2043, 0.0000, 0.0029],\n",
      "         [0.0000, 0.6100, 0.2240,  ..., 0.2534, 0.0000, 0.2684],\n",
      "         [0.0859, 0.0000, 0.4089,  ..., 0.3956, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0124],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0000],\n",
      "         [0.0000, 1.1501, 0.5173,  ..., 0.6002, 0.0000, 0.0000]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "AFTER CHANNEL NORM SPEAKER: tensor([[[-0.6577,  0.7007, -0.6895,  ..., -0.5958, -0.3284, -0.6076],\n",
      "         [-0.3186,  3.7963,  1.2480,  ..., -0.5958, -0.6345,  1.1988],\n",
      "         [-0.6577, -0.6762, -0.6895,  ...,  1.5816,  5.2845,  2.5574],\n",
      "         ...,\n",
      "         [-0.6577, -0.6762, -0.6895,  ..., -0.5958, -0.6345, -0.6076],\n",
      "         [ 1.0351,  0.6182, -0.6895,  ..., -0.5958, -0.6345,  1.3817],\n",
      "         [-0.6577,  0.1833,  0.8812,  ...,  1.6955,  0.2170, -0.1892]],\n",
      "\n",
      "        [[ 1.3787, -0.6515, -0.6576,  ...,  0.7104,  2.2550, -0.6001],\n",
      "         [-0.0877, -0.2936, -0.6576,  ..., -0.6423, -0.6372, -0.6001],\n",
      "         [ 0.6864, -0.6515,  0.7898,  ...,  3.8509, -0.6372,  1.4571],\n",
      "         ...,\n",
      "         [-0.2074, -0.6515, -0.5040,  ..., -0.6423, -0.1259, -0.6001],\n",
      "         [-0.7177,  2.3094, -0.6576,  ...,  1.0001, -0.6372, -0.6001],\n",
      "         [-0.7177, -0.6515,  1.2149,  ..., -0.6423,  1.4277, -0.4450]],\n",
      "\n",
      "        [[-0.6678,  0.2566,  0.6228,  ...,  1.1755, -0.0388, -0.5851],\n",
      "         [ 0.6579, -0.6737, -0.6630,  ...,  1.3140, -0.6594, -0.5851],\n",
      "         [ 3.2438, -0.0222,  1.2064,  ...,  1.7972,  1.5820,  1.0831],\n",
      "         ...,\n",
      "         [-0.6678, -0.6737, -0.6630,  ..., -0.6407, -0.6594, -0.5282],\n",
      "         [-0.6678, -0.6737, -0.6630,  ..., -0.6326, -0.6594, -0.5851],\n",
      "         [-0.6678,  3.4135,  1.0935,  ...,  1.5490, -0.6594, -0.5851]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "AFTER 1x1 CONV: tensor([[[-0.1822,  0.6182, -0.7846,  ...,  0.2745,  1.1946,  0.6106],\n",
      "         [ 0.8146,  0.4940, -0.5286,  ...,  0.3101, -0.0315,  0.2522],\n",
      "         [ 0.0128, -0.1692, -0.0095,  ..., -0.1994,  0.7362,  0.0703],\n",
      "         ...,\n",
      "         [-0.3052,  0.0396, -0.7063,  ...,  1.0130, -1.7290, -0.2205],\n",
      "         [ 0.3425,  0.0299,  0.8897,  ..., -0.8929, -0.6212,  0.8535],\n",
      "         [-0.8264, -0.1488,  0.2717,  ...,  1.6383, -0.6234, -1.3460]],\n",
      "\n",
      "        [[ 0.1685, -0.7898, -0.4072,  ..., -0.0895, -0.1672,  0.3627],\n",
      "         [-0.4603, -1.2275, -0.0210,  ...,  0.3106,  0.6057, -0.6672],\n",
      "         [-0.6712, -0.3040, -1.3474,  ...,  0.4433, -0.1010, -0.2716],\n",
      "         ...,\n",
      "         [ 0.6249, -0.1994, -0.8163,  ...,  0.1534, -0.5086, -0.4112],\n",
      "         [-0.4182,  0.2268, -1.0154,  ..., -0.5967, -0.3714,  0.0163],\n",
      "         [-0.5672,  0.1102,  0.3510,  ...,  0.8227, -0.4012, -0.0376]],\n",
      "\n",
      "        [[-0.0882,  0.8457,  0.2260,  ..., -0.4515,  0.0532, -0.3654],\n",
      "         [-0.2977, -0.6321, -0.0025,  ..., -0.6363, -0.6680, -0.6001],\n",
      "         [-0.9681,  0.2023, -0.1793,  ...,  0.0084, -0.2998, -0.4883],\n",
      "         ...,\n",
      "         [ 0.8768, -0.4903, -0.4995,  ...,  0.1811, -0.3358, -0.9000],\n",
      "         [ 0.3111, -0.0567, -0.8072,  ..., -0.5163, -0.1247, -0.6607],\n",
      "         [ 1.0128,  0.3963, -0.4308,  ...,  0.3907, -0.3328, -0.1323]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "AFTER RESNET: tensor([[[-2.0259],\n",
      "         [-0.2838],\n",
      "         [ 0.0604],\n",
      "         [ 1.2440],\n",
      "         [ 0.4398],\n",
      "         [-1.5963],\n",
      "         [-0.2039],\n",
      "         [ 0.5470],\n",
      "         [-0.8555],\n",
      "         [-1.9634],\n",
      "         [-0.4123],\n",
      "         [ 0.2807],\n",
      "         [-0.4909],\n",
      "         [-3.0769],\n",
      "         [ 0.5207],\n",
      "         [ 0.9470],\n",
      "         [ 1.8354],\n",
      "         [ 1.4997],\n",
      "         [ 0.0211],\n",
      "         [-0.7642],\n",
      "         [ 1.2300],\n",
      "         [ 0.1932],\n",
      "         [ 1.0708],\n",
      "         [-1.0552],\n",
      "         [-1.3784],\n",
      "         [ 0.8540],\n",
      "         [ 1.4754],\n",
      "         [-1.0237],\n",
      "         [ 1.2748],\n",
      "         [-1.2684],\n",
      "         [ 1.1358],\n",
      "         [-0.6333],\n",
      "         [-0.0514],\n",
      "         [-0.4810],\n",
      "         [-1.0054],\n",
      "         [-0.5435],\n",
      "         [-0.1389],\n",
      "         [-0.4141],\n",
      "         [ 1.2723],\n",
      "         [-0.6519],\n",
      "         [-0.8523],\n",
      "         [ 0.3440],\n",
      "         [ 2.2404],\n",
      "         [ 0.4998],\n",
      "         [-1.4383],\n",
      "         [-1.9270],\n",
      "         [ 0.2345],\n",
      "         [-0.2418],\n",
      "         [ 2.0361],\n",
      "         [ 0.2110],\n",
      "         [-1.4227],\n",
      "         [-0.6032],\n",
      "         [ 0.6999],\n",
      "         [-1.7142],\n",
      "         [-1.3662],\n",
      "         [ 1.0317],\n",
      "         [-0.2807],\n",
      "         [-0.4482],\n",
      "         [ 1.8598],\n",
      "         [ 0.1740],\n",
      "         [ 1.0650],\n",
      "         [-0.7824],\n",
      "         [ 0.5451],\n",
      "         [-0.0216],\n",
      "         [-0.4666],\n",
      "         [-0.0541],\n",
      "         [ 0.2124],\n",
      "         [ 2.2101],\n",
      "         [ 0.3158],\n",
      "         [-1.5608],\n",
      "         [-0.0626],\n",
      "         [ 2.7314],\n",
      "         [-1.9299],\n",
      "         [ 0.4547],\n",
      "         [-0.1399],\n",
      "         [ 1.8476],\n",
      "         [ 0.6432],\n",
      "         [ 2.1536],\n",
      "         [-0.8954],\n",
      "         [-1.0244],\n",
      "         [ 1.5778],\n",
      "         [ 1.2976],\n",
      "         [ 0.5680],\n",
      "         [ 0.3518],\n",
      "         [-1.1428],\n",
      "         [-0.0916],\n",
      "         [ 0.6457],\n",
      "         [-1.7563],\n",
      "         [ 2.2487],\n",
      "         [ 1.2442],\n",
      "         [-0.6806],\n",
      "         [ 0.6044],\n",
      "         [ 0.2930],\n",
      "         [-0.0766],\n",
      "         [-0.4885],\n",
      "         [-0.1052],\n",
      "         [ 0.8510],\n",
      "         [ 1.2984],\n",
      "         [ 0.6516],\n",
      "         [-0.4267],\n",
      "         [ 2.1976],\n",
      "         [ 1.2804],\n",
      "         [-0.6981],\n",
      "         [ 0.2361],\n",
      "         [ 2.1558],\n",
      "         [ 0.6908],\n",
      "         [-1.1581],\n",
      "         [-3.0930],\n",
      "         [ 0.2385],\n",
      "         [ 1.6028],\n",
      "         [ 1.4582],\n",
      "         [-0.9028],\n",
      "         [ 0.6662],\n",
      "         [-0.0763],\n",
      "         [-0.7684],\n",
      "         [ 0.7638],\n",
      "         [ 0.3600],\n",
      "         [-0.3218],\n",
      "         [-1.1313],\n",
      "         [-1.1868],\n",
      "         [-3.2087],\n",
      "         [-1.6886],\n",
      "         [ 0.2219],\n",
      "         [ 0.9707],\n",
      "         [-1.0496],\n",
      "         [ 1.2133],\n",
      "         [ 0.8579],\n",
      "         [-1.5565],\n",
      "         [ 0.9540],\n",
      "         [ 0.6537],\n",
      "         [ 0.9677],\n",
      "         [ 0.7901],\n",
      "         [-0.7254],\n",
      "         [-2.0446],\n",
      "         [-0.2400],\n",
      "         [-1.1944],\n",
      "         [-2.4322],\n",
      "         [ 1.2344],\n",
      "         [-0.7454],\n",
      "         [ 1.0831],\n",
      "         [-0.1047],\n",
      "         [-1.1457],\n",
      "         [ 2.0079],\n",
      "         [ 1.8745],\n",
      "         [ 0.5349],\n",
      "         [ 1.5190],\n",
      "         [-0.6457],\n",
      "         [ 0.6903],\n",
      "         [ 0.7211],\n",
      "         [-1.3232],\n",
      "         [-2.0473],\n",
      "         [ 2.0651],\n",
      "         [ 0.3935],\n",
      "         [ 0.9992],\n",
      "         [-1.0175],\n",
      "         [ 3.5896],\n",
      "         [ 1.0266],\n",
      "         [ 1.3133],\n",
      "         [-1.4591],\n",
      "         [-2.7154],\n",
      "         [ 1.3666],\n",
      "         [ 1.6375],\n",
      "         [ 1.7197],\n",
      "         [ 0.0380],\n",
      "         [-0.7634],\n",
      "         [-0.9335],\n",
      "         [ 0.9654],\n",
      "         [-0.2959],\n",
      "         [ 0.6161],\n",
      "         [-0.5573],\n",
      "         [-1.4403],\n",
      "         [ 0.9945],\n",
      "         [ 0.5428],\n",
      "         [-0.0297],\n",
      "         [-0.9736],\n",
      "         [-0.4641],\n",
      "         [-0.5099],\n",
      "         [ 0.1635],\n",
      "         [-1.8866],\n",
      "         [ 0.7940],\n",
      "         [ 0.8291],\n",
      "         [-1.5168],\n",
      "         [-1.2570],\n",
      "         [ 1.7462],\n",
      "         [ 1.0290],\n",
      "         [ 1.2585],\n",
      "         [ 0.8228],\n",
      "         [-0.0834],\n",
      "         [ 0.6656],\n",
      "         [-0.9005],\n",
      "         [-1.6750],\n",
      "         [-0.0448],\n",
      "         [-1.0103],\n",
      "         [-0.1653],\n",
      "         [-0.7649],\n",
      "         [ 0.0806],\n",
      "         [-0.3401],\n",
      "         [-0.7580],\n",
      "         [-2.2858],\n",
      "         [ 1.4037],\n",
      "         [-0.9240],\n",
      "         [-0.8823],\n",
      "         [-1.3984],\n",
      "         [ 0.0311],\n",
      "         [ 0.1403],\n",
      "         [ 1.3854],\n",
      "         [-0.7344],\n",
      "         [ 1.2887],\n",
      "         [-0.6261],\n",
      "         [-0.0439],\n",
      "         [-1.0491],\n",
      "         [-1.4793],\n",
      "         [-0.4341],\n",
      "         [ 0.9444],\n",
      "         [ 1.0156],\n",
      "         [-1.1332],\n",
      "         [-0.6626],\n",
      "         [-0.9058],\n",
      "         [ 0.6407],\n",
      "         [ 1.3388],\n",
      "         [ 1.5398],\n",
      "         [-1.4044],\n",
      "         [ 0.3002],\n",
      "         [ 0.7005],\n",
      "         [-0.8541],\n",
      "         [ 0.3078],\n",
      "         [-1.3089],\n",
      "         [-0.4579],\n",
      "         [ 0.8489],\n",
      "         [-1.6015],\n",
      "         [-0.2139],\n",
      "         [-0.1828],\n",
      "         [ 1.2954],\n",
      "         [ 0.4924],\n",
      "         [-0.2582],\n",
      "         [-1.6401],\n",
      "         [ 1.3238],\n",
      "         [ 0.5880],\n",
      "         [ 0.2673],\n",
      "         [-1.8540],\n",
      "         [ 0.9338],\n",
      "         [ 0.6087],\n",
      "         [ 2.9167],\n",
      "         [-1.0555],\n",
      "         [-2.0464],\n",
      "         [ 2.7195],\n",
      "         [ 0.4362],\n",
      "         [ 1.5167],\n",
      "         [ 0.4473],\n",
      "         [-0.2384],\n",
      "         [ 1.0956],\n",
      "         [ 0.1541],\n",
      "         [-0.3029],\n",
      "         [ 0.0676],\n",
      "         [-0.9080],\n",
      "         [-1.4110]],\n",
      "\n",
      "        [[-1.4182],\n",
      "         [-0.2182],\n",
      "         [-0.1023],\n",
      "         [-0.4267],\n",
      "         [-1.3960],\n",
      "         [-1.3806],\n",
      "         [ 1.8740],\n",
      "         [ 0.1044],\n",
      "         [-1.2336],\n",
      "         [-1.9951],\n",
      "         [ 0.2577],\n",
      "         [ 1.2588],\n",
      "         [-0.1353],\n",
      "         [-2.1796],\n",
      "         [-0.7904],\n",
      "         [ 0.0448],\n",
      "         [ 2.0533],\n",
      "         [-0.0949],\n",
      "         [ 0.6662],\n",
      "         [-0.3766],\n",
      "         [ 1.0668],\n",
      "         [-0.0396],\n",
      "         [ 1.0215],\n",
      "         [-1.9341],\n",
      "         [-0.7466],\n",
      "         [ 0.5928],\n",
      "         [ 0.5746],\n",
      "         [-1.0885],\n",
      "         [-0.5074],\n",
      "         [-0.8082],\n",
      "         [ 1.0262],\n",
      "         [-0.3347],\n",
      "         [ 0.4916],\n",
      "         [-0.0567],\n",
      "         [-1.1743],\n",
      "         [-0.4261],\n",
      "         [-1.1648],\n",
      "         [-0.1092],\n",
      "         [ 2.4492],\n",
      "         [-0.7325],\n",
      "         [-1.6105],\n",
      "         [-0.5188],\n",
      "         [ 1.9732],\n",
      "         [ 0.1232],\n",
      "         [-0.7427],\n",
      "         [ 0.4167],\n",
      "         [-0.8692],\n",
      "         [ 0.5324],\n",
      "         [ 1.6281],\n",
      "         [-0.3154],\n",
      "         [-2.1177],\n",
      "         [ 0.3753],\n",
      "         [-0.1180],\n",
      "         [-0.7145],\n",
      "         [-1.7397],\n",
      "         [-0.1826],\n",
      "         [-0.0371],\n",
      "         [-0.2580],\n",
      "         [ 0.6103],\n",
      "         [ 1.4456],\n",
      "         [ 1.3040],\n",
      "         [-1.1309],\n",
      "         [ 2.0776],\n",
      "         [-0.3606],\n",
      "         [-0.0670],\n",
      "         [-0.0332],\n",
      "         [ 0.3264],\n",
      "         [ 1.8684],\n",
      "         [ 0.7679],\n",
      "         [-1.1749],\n",
      "         [-0.9432],\n",
      "         [ 2.6970],\n",
      "         [-0.8841],\n",
      "         [ 0.9681],\n",
      "         [ 0.3468],\n",
      "         [ 1.1400],\n",
      "         [ 0.3183],\n",
      "         [ 0.3113],\n",
      "         [-1.5308],\n",
      "         [-1.2741],\n",
      "         [ 0.7175],\n",
      "         [ 0.0778],\n",
      "         [ 1.4521],\n",
      "         [-0.6220],\n",
      "         [-0.8268],\n",
      "         [-1.2775],\n",
      "         [ 1.2206],\n",
      "         [-1.7646],\n",
      "         [ 1.4184],\n",
      "         [ 0.8296],\n",
      "         [-0.7049],\n",
      "         [-0.8948],\n",
      "         [ 0.3066],\n",
      "         [-0.0570],\n",
      "         [ 0.0635],\n",
      "         [-0.9630],\n",
      "         [-0.0839],\n",
      "         [ 0.3416],\n",
      "         [-0.2927],\n",
      "         [-0.3839],\n",
      "         [ 2.6632],\n",
      "         [ 2.1682],\n",
      "         [ 1.0210],\n",
      "         [ 1.7116],\n",
      "         [ 2.6585],\n",
      "         [ 0.5921],\n",
      "         [-1.7967],\n",
      "         [-2.9018],\n",
      "         [ 0.1788],\n",
      "         [ 1.2579],\n",
      "         [ 0.2216],\n",
      "         [-0.9169],\n",
      "         [ 1.0273],\n",
      "         [ 0.2968],\n",
      "         [-1.3423],\n",
      "         [ 0.4537],\n",
      "         [-0.1279],\n",
      "         [-0.3671],\n",
      "         [-0.8311],\n",
      "         [-2.9004],\n",
      "         [-3.8374],\n",
      "         [-0.9702],\n",
      "         [ 0.4894],\n",
      "         [ 1.2095],\n",
      "         [-0.7004],\n",
      "         [ 0.7646],\n",
      "         [ 0.5561],\n",
      "         [-1.5431],\n",
      "         [ 1.0439],\n",
      "         [ 0.1865],\n",
      "         [ 1.7795],\n",
      "         [ 1.2320],\n",
      "         [-1.4066],\n",
      "         [-2.0637],\n",
      "         [ 0.2312],\n",
      "         [-0.4524],\n",
      "         [-0.7335],\n",
      "         [ 1.8048],\n",
      "         [-1.5055],\n",
      "         [ 1.0635],\n",
      "         [-0.2231],\n",
      "         [-0.4206],\n",
      "         [ 2.1401],\n",
      "         [ 2.0623],\n",
      "         [ 1.5435],\n",
      "         [ 1.4956],\n",
      "         [ 0.0160],\n",
      "         [-0.0484],\n",
      "         [ 1.7636],\n",
      "         [-0.3204],\n",
      "         [-2.2655],\n",
      "         [ 1.9654],\n",
      "         [-0.1558],\n",
      "         [ 1.9818],\n",
      "         [-0.7916],\n",
      "         [ 3.9763],\n",
      "         [ 2.2148],\n",
      "         [ 0.8945],\n",
      "         [-1.5890],\n",
      "         [-2.0303],\n",
      "         [ 0.2727],\n",
      "         [ 1.8455],\n",
      "         [ 2.9843],\n",
      "         [-0.8028],\n",
      "         [-0.3850],\n",
      "         [ 0.5318],\n",
      "         [ 0.6897],\n",
      "         [-0.6176],\n",
      "         [ 1.2110],\n",
      "         [-0.2987],\n",
      "         [ 0.2481],\n",
      "         [ 0.7692],\n",
      "         [ 0.4880],\n",
      "         [ 1.1342],\n",
      "         [-1.6001],\n",
      "         [-0.6336],\n",
      "         [-1.5759],\n",
      "         [-0.1944],\n",
      "         [-1.5333],\n",
      "         [ 0.8422],\n",
      "         [ 0.0225],\n",
      "         [-1.4306],\n",
      "         [-1.0241],\n",
      "         [ 1.8493],\n",
      "         [ 0.7229],\n",
      "         [-0.3454],\n",
      "         [-0.5266],\n",
      "         [-1.2375],\n",
      "         [ 2.0908],\n",
      "         [-0.3211],\n",
      "         [-0.7369],\n",
      "         [ 0.0577],\n",
      "         [-0.8805],\n",
      "         [-1.0478],\n",
      "         [ 0.3421],\n",
      "         [-0.2789],\n",
      "         [-0.9346],\n",
      "         [-0.2239],\n",
      "         [-3.1391],\n",
      "         [ 2.3096],\n",
      "         [-1.3037],\n",
      "         [-0.5874],\n",
      "         [-2.5349],\n",
      "         [ 1.0622],\n",
      "         [ 0.2184],\n",
      "         [ 0.2052],\n",
      "         [-1.0632],\n",
      "         [ 0.4440],\n",
      "         [-0.3334],\n",
      "         [ 0.0454],\n",
      "         [-0.6391],\n",
      "         [-1.8672],\n",
      "         [ 0.6086],\n",
      "         [-0.6256],\n",
      "         [ 0.8159],\n",
      "         [-1.2496],\n",
      "         [-0.8402],\n",
      "         [-1.1662],\n",
      "         [ 2.7419],\n",
      "         [-0.8626],\n",
      "         [ 1.3446],\n",
      "         [-1.8557],\n",
      "         [ 0.2555],\n",
      "         [ 1.5807],\n",
      "         [-0.8134],\n",
      "         [-0.9551],\n",
      "         [-1.5794],\n",
      "         [-1.8157],\n",
      "         [ 1.5301],\n",
      "         [-1.4135],\n",
      "         [-0.4456],\n",
      "         [-0.8424],\n",
      "         [ 0.9663],\n",
      "         [ 0.0538],\n",
      "         [-1.5965],\n",
      "         [-1.7118],\n",
      "         [ 0.8216],\n",
      "         [ 2.2135],\n",
      "         [ 0.2181],\n",
      "         [-0.7249],\n",
      "         [ 0.6337],\n",
      "         [ 0.3060],\n",
      "         [ 2.4481],\n",
      "         [ 0.7957],\n",
      "         [-2.2571],\n",
      "         [ 1.7426],\n",
      "         [ 0.7518],\n",
      "         [ 2.5588],\n",
      "         [ 0.3605],\n",
      "         [ 0.2382],\n",
      "         [ 0.9165],\n",
      "         [ 1.1561],\n",
      "         [ 0.0592],\n",
      "         [ 0.0852],\n",
      "         [-3.1272],\n",
      "         [-1.5849]],\n",
      "\n",
      "        [[-2.2071],\n",
      "         [ 0.3996],\n",
      "         [ 2.5439],\n",
      "         [ 0.0165],\n",
      "         [-1.7567],\n",
      "         [-1.4408],\n",
      "         [ 0.7840],\n",
      "         [ 0.9452],\n",
      "         [-1.6737],\n",
      "         [-1.2458],\n",
      "         [ 0.3420],\n",
      "         [ 0.5990],\n",
      "         [ 0.4563],\n",
      "         [-2.4572],\n",
      "         [ 0.1409],\n",
      "         [ 1.1976],\n",
      "         [ 2.0539],\n",
      "         [ 0.8490],\n",
      "         [-0.4046],\n",
      "         [-0.1258],\n",
      "         [ 1.0922],\n",
      "         [-0.0835],\n",
      "         [ 1.0964],\n",
      "         [-0.6058],\n",
      "         [-0.6084],\n",
      "         [ 1.5896],\n",
      "         [-0.1303],\n",
      "         [-0.1760],\n",
      "         [-0.1585],\n",
      "         [-1.7712],\n",
      "         [ 2.0001],\n",
      "         [-1.1252],\n",
      "         [ 0.3986],\n",
      "         [-0.3373],\n",
      "         [-0.1778],\n",
      "         [ 0.5467],\n",
      "         [ 0.2682],\n",
      "         [-0.0878],\n",
      "         [ 2.3811],\n",
      "         [-1.6552],\n",
      "         [-1.8807],\n",
      "         [-0.8919],\n",
      "         [ 2.0184],\n",
      "         [ 0.2141],\n",
      "         [-1.4197],\n",
      "         [-1.7058],\n",
      "         [-0.3446],\n",
      "         [-0.6886],\n",
      "         [ 2.1675],\n",
      "         [ 0.5360],\n",
      "         [-2.9155],\n",
      "         [ 1.4698],\n",
      "         [ 0.1327],\n",
      "         [-1.1086],\n",
      "         [-2.7736],\n",
      "         [ 0.2878],\n",
      "         [-0.1584],\n",
      "         [-0.7231],\n",
      "         [ 1.7642],\n",
      "         [ 0.4261],\n",
      "         [ 1.5665],\n",
      "         [-1.1526],\n",
      "         [ 2.4705],\n",
      "         [ 0.5894],\n",
      "         [-0.3344],\n",
      "         [-0.3484],\n",
      "         [ 0.7802],\n",
      "         [ 0.6880],\n",
      "         [ 1.0666],\n",
      "         [-1.9429],\n",
      "         [-1.1394],\n",
      "         [ 3.2015],\n",
      "         [-1.1595],\n",
      "         [ 0.3946],\n",
      "         [ 0.4305],\n",
      "         [ 0.5851],\n",
      "         [-0.4886],\n",
      "         [ 1.2316],\n",
      "         [-0.8091],\n",
      "         [-0.5557],\n",
      "         [ 1.2102],\n",
      "         [ 1.1299],\n",
      "         [ 0.9386],\n",
      "         [-0.7356],\n",
      "         [-0.6788],\n",
      "         [-0.7780],\n",
      "         [ 1.4530],\n",
      "         [-1.2951],\n",
      "         [ 1.9818],\n",
      "         [ 0.3565],\n",
      "         [-0.9663],\n",
      "         [-0.9464],\n",
      "         [-0.8874],\n",
      "         [ 1.2613],\n",
      "         [-0.7002],\n",
      "         [-0.4634],\n",
      "         [ 0.0629],\n",
      "         [ 0.2232],\n",
      "         [ 1.5468],\n",
      "         [ 0.3711],\n",
      "         [ 2.0030],\n",
      "         [ 0.8442],\n",
      "         [ 0.3566],\n",
      "         [ 0.2267],\n",
      "         [ 1.6987],\n",
      "         [ 1.2690],\n",
      "         [-2.2269],\n",
      "         [-3.0458],\n",
      "         [ 0.4215],\n",
      "         [ 1.7313],\n",
      "         [-0.1811],\n",
      "         [ 0.4244],\n",
      "         [-0.4334],\n",
      "         [-0.5803],\n",
      "         [-0.1558],\n",
      "         [ 0.2169],\n",
      "         [ 0.6433],\n",
      "         [-1.5644],\n",
      "         [-0.8113],\n",
      "         [-2.6462],\n",
      "         [-4.2973],\n",
      "         [-0.7942],\n",
      "         [ 0.5222],\n",
      "         [ 1.2436],\n",
      "         [-0.7390],\n",
      "         [ 1.6474],\n",
      "         [ 0.5913],\n",
      "         [-0.1062],\n",
      "         [ 1.2031],\n",
      "         [-0.0497],\n",
      "         [ 0.6177],\n",
      "         [ 2.2086],\n",
      "         [-1.9235],\n",
      "         [-1.0945],\n",
      "         [ 0.4109],\n",
      "         [-1.0124],\n",
      "         [-1.4265],\n",
      "         [ 1.0111],\n",
      "         [-0.6632],\n",
      "         [ 0.1246],\n",
      "         [-1.0842],\n",
      "         [-1.7413],\n",
      "         [ 1.8300],\n",
      "         [ 2.0742],\n",
      "         [ 0.1854],\n",
      "         [ 1.5697],\n",
      "         [-0.6398],\n",
      "         [-0.2570],\n",
      "         [ 2.5513],\n",
      "         [-1.6643],\n",
      "         [-2.2194],\n",
      "         [ 2.1767],\n",
      "         [ 0.5770],\n",
      "         [ 0.8826],\n",
      "         [-1.2543],\n",
      "         [ 3.3159],\n",
      "         [ 0.9286],\n",
      "         [ 0.3401],\n",
      "         [-1.9444],\n",
      "         [-1.2438],\n",
      "         [ 0.1403],\n",
      "         [ 2.9119],\n",
      "         [ 1.5603],\n",
      "         [ 1.0041],\n",
      "         [-0.7940],\n",
      "         [-0.4264],\n",
      "         [ 1.4307],\n",
      "         [-0.3661],\n",
      "         [-0.1408],\n",
      "         [ 1.1232],\n",
      "         [-0.9220],\n",
      "         [ 2.0510],\n",
      "         [ 0.2441],\n",
      "         [ 0.1441],\n",
      "         [-0.8899],\n",
      "         [-0.3884],\n",
      "         [-1.1081],\n",
      "         [ 0.3796],\n",
      "         [-1.6611],\n",
      "         [-0.4975],\n",
      "         [ 0.9028],\n",
      "         [-0.3793],\n",
      "         [-2.1230],\n",
      "         [ 1.7233],\n",
      "         [ 1.1491],\n",
      "         [ 1.8381],\n",
      "         [-0.1276],\n",
      "         [-0.5506],\n",
      "         [ 1.9022],\n",
      "         [-0.8074],\n",
      "         [-0.7385],\n",
      "         [-0.4844],\n",
      "         [-1.1592],\n",
      "         [-1.3967],\n",
      "         [-0.1325],\n",
      "         [ 0.2027],\n",
      "         [-0.8594],\n",
      "         [ 0.2639],\n",
      "         [-2.2602],\n",
      "         [ 1.8508],\n",
      "         [-0.2133],\n",
      "         [-0.2329],\n",
      "         [-2.6417],\n",
      "         [-0.1733],\n",
      "         [ 1.2263],\n",
      "         [ 1.0406],\n",
      "         [-1.0850],\n",
      "         [ 0.6321],\n",
      "         [-0.1211],\n",
      "         [-0.9953],\n",
      "         [-0.7337],\n",
      "         [-1.4925],\n",
      "         [ 0.4646],\n",
      "         [ 1.5566],\n",
      "         [ 0.9375],\n",
      "         [-0.9978],\n",
      "         [ 0.4407],\n",
      "         [-0.3877],\n",
      "         [ 1.9274],\n",
      "         [ 0.9547],\n",
      "         [ 1.0424],\n",
      "         [-1.6345],\n",
      "         [-0.8823],\n",
      "         [ 0.2048],\n",
      "         [-0.1532],\n",
      "         [ 0.5478],\n",
      "         [-1.3781],\n",
      "         [-1.3997],\n",
      "         [ 0.9459],\n",
      "         [-1.6794],\n",
      "         [-0.2210],\n",
      "         [-0.1860],\n",
      "         [ 1.3499],\n",
      "         [ 0.2578],\n",
      "         [-1.5932],\n",
      "         [-3.3907],\n",
      "         [ 0.9961],\n",
      "         [ 1.0839],\n",
      "         [-0.3438],\n",
      "         [-0.9135],\n",
      "         [ 0.1724],\n",
      "         [ 0.8975],\n",
      "         [ 3.2153],\n",
      "         [-0.4255],\n",
      "         [-1.8605],\n",
      "         [ 1.8480],\n",
      "         [-0.8373],\n",
      "         [ 2.8388],\n",
      "         [ 1.1463],\n",
      "         [-0.8250],\n",
      "         [ 0.8935],\n",
      "         [-1.2161],\n",
      "         [-0.9631],\n",
      "         [ 0.1710],\n",
      "         [-2.1118],\n",
      "         [-2.6569]]], grad_fn=<ConvolutionBackward0>)\n",
      "AVERAGE POOLING: tensor([[-2.0259, -0.2838,  0.0604,  1.2440,  0.4398, -1.5963, -0.2039,  0.5470,\n",
      "         -0.8555, -1.9634, -0.4123,  0.2807, -0.4909, -3.0769,  0.5207,  0.9470,\n",
      "          1.8354,  1.4997,  0.0211, -0.7642,  1.2300,  0.1932,  1.0708, -1.0552,\n",
      "         -1.3784,  0.8540,  1.4754, -1.0237,  1.2748, -1.2684,  1.1358, -0.6333,\n",
      "         -0.0514, -0.4810, -1.0054, -0.5435, -0.1389, -0.4141,  1.2723, -0.6519,\n",
      "         -0.8523,  0.3440,  2.2404,  0.4998, -1.4383, -1.9270,  0.2345, -0.2418,\n",
      "          2.0361,  0.2110, -1.4227, -0.6032,  0.6999, -1.7142, -1.3662,  1.0317,\n",
      "         -0.2807, -0.4482,  1.8598,  0.1740,  1.0650, -0.7824,  0.5451, -0.0216,\n",
      "         -0.4666, -0.0541,  0.2124,  2.2101,  0.3158, -1.5608, -0.0626,  2.7314,\n",
      "         -1.9299,  0.4547, -0.1399,  1.8476,  0.6432,  2.1536, -0.8954, -1.0244,\n",
      "          1.5778,  1.2976,  0.5680,  0.3518, -1.1428, -0.0916,  0.6457, -1.7563,\n",
      "          2.2487,  1.2442, -0.6806,  0.6044,  0.2930, -0.0766, -0.4885, -0.1052,\n",
      "          0.8510,  1.2984,  0.6516, -0.4267,  2.1976,  1.2804, -0.6981,  0.2361,\n",
      "          2.1558,  0.6908, -1.1581, -3.0930,  0.2385,  1.6028,  1.4582, -0.9028,\n",
      "          0.6662, -0.0763, -0.7684,  0.7638,  0.3600, -0.3218, -1.1313, -1.1868,\n",
      "         -3.2087, -1.6886,  0.2219,  0.9707, -1.0496,  1.2133,  0.8579, -1.5565,\n",
      "          0.9540,  0.6537,  0.9677,  0.7901, -0.7254, -2.0446, -0.2400, -1.1944,\n",
      "         -2.4322,  1.2344, -0.7454,  1.0831, -0.1047, -1.1457,  2.0079,  1.8745,\n",
      "          0.5349,  1.5190, -0.6457,  0.6903,  0.7211, -1.3232, -2.0473,  2.0651,\n",
      "          0.3935,  0.9992, -1.0175,  3.5896,  1.0266,  1.3133, -1.4591, -2.7154,\n",
      "          1.3666,  1.6375,  1.7197,  0.0380, -0.7634, -0.9335,  0.9654, -0.2959,\n",
      "          0.6161, -0.5573, -1.4403,  0.9945,  0.5428, -0.0297, -0.9736, -0.4641,\n",
      "         -0.5099,  0.1635, -1.8866,  0.7940,  0.8291, -1.5168, -1.2570,  1.7462,\n",
      "          1.0290,  1.2585,  0.8228, -0.0834,  0.6656, -0.9005, -1.6750, -0.0448,\n",
      "         -1.0103, -0.1653, -0.7649,  0.0806, -0.3401, -0.7580, -2.2858,  1.4037,\n",
      "         -0.9240, -0.8823, -1.3984,  0.0311,  0.1403,  1.3854, -0.7344,  1.2887,\n",
      "         -0.6261, -0.0439, -1.0491, -1.4793, -0.4341,  0.9444,  1.0156, -1.1332,\n",
      "         -0.6626, -0.9058,  0.6407,  1.3388,  1.5398, -1.4044,  0.3002,  0.7005,\n",
      "         -0.8541,  0.3078, -1.3089, -0.4579,  0.8489, -1.6015, -0.2139, -0.1828,\n",
      "          1.2954,  0.4924, -0.2582, -1.6401,  1.3238,  0.5880,  0.2673, -1.8540,\n",
      "          0.9338,  0.6087,  2.9167, -1.0555, -2.0464,  2.7195,  0.4362,  1.5167,\n",
      "          0.4473, -0.2384,  1.0956,  0.1541, -0.3029,  0.0676, -0.9080, -1.4110],\n",
      "        [-1.4182, -0.2182, -0.1023, -0.4267, -1.3960, -1.3806,  1.8740,  0.1044,\n",
      "         -1.2336, -1.9951,  0.2577,  1.2588, -0.1353, -2.1796, -0.7904,  0.0448,\n",
      "          2.0533, -0.0949,  0.6662, -0.3766,  1.0668, -0.0396,  1.0215, -1.9341,\n",
      "         -0.7466,  0.5928,  0.5746, -1.0885, -0.5074, -0.8082,  1.0262, -0.3347,\n",
      "          0.4916, -0.0567, -1.1743, -0.4261, -1.1648, -0.1092,  2.4492, -0.7325,\n",
      "         -1.6105, -0.5188,  1.9732,  0.1232, -0.7427,  0.4167, -0.8692,  0.5324,\n",
      "          1.6281, -0.3154, -2.1177,  0.3753, -0.1180, -0.7145, -1.7397, -0.1826,\n",
      "         -0.0371, -0.2580,  0.6103,  1.4456,  1.3040, -1.1309,  2.0776, -0.3606,\n",
      "         -0.0670, -0.0332,  0.3264,  1.8684,  0.7679, -1.1749, -0.9432,  2.6970,\n",
      "         -0.8841,  0.9681,  0.3468,  1.1400,  0.3183,  0.3113, -1.5308, -1.2741,\n",
      "          0.7175,  0.0778,  1.4521, -0.6220, -0.8268, -1.2775,  1.2206, -1.7646,\n",
      "          1.4184,  0.8296, -0.7049, -0.8948,  0.3066, -0.0570,  0.0635, -0.9630,\n",
      "         -0.0839,  0.3416, -0.2927, -0.3839,  2.6632,  2.1682,  1.0210,  1.7116,\n",
      "          2.6585,  0.5921, -1.7967, -2.9018,  0.1788,  1.2579,  0.2216, -0.9169,\n",
      "          1.0273,  0.2968, -1.3423,  0.4537, -0.1279, -0.3671, -0.8311, -2.9004,\n",
      "         -3.8374, -0.9702,  0.4894,  1.2095, -0.7004,  0.7646,  0.5561, -1.5431,\n",
      "          1.0439,  0.1865,  1.7795,  1.2320, -1.4066, -2.0637,  0.2312, -0.4524,\n",
      "         -0.7335,  1.8048, -1.5055,  1.0635, -0.2231, -0.4206,  2.1401,  2.0623,\n",
      "          1.5435,  1.4956,  0.0160, -0.0484,  1.7636, -0.3204, -2.2655,  1.9654,\n",
      "         -0.1558,  1.9818, -0.7916,  3.9763,  2.2148,  0.8945, -1.5890, -2.0303,\n",
      "          0.2727,  1.8455,  2.9843, -0.8028, -0.3850,  0.5318,  0.6897, -0.6176,\n",
      "          1.2110, -0.2987,  0.2481,  0.7692,  0.4880,  1.1342, -1.6001, -0.6336,\n",
      "         -1.5759, -0.1944, -1.5333,  0.8422,  0.0225, -1.4306, -1.0241,  1.8493,\n",
      "          0.7229, -0.3454, -0.5266, -1.2375,  2.0908, -0.3211, -0.7369,  0.0577,\n",
      "         -0.8805, -1.0478,  0.3421, -0.2789, -0.9346, -0.2239, -3.1391,  2.3096,\n",
      "         -1.3037, -0.5874, -2.5349,  1.0622,  0.2184,  0.2052, -1.0632,  0.4440,\n",
      "         -0.3334,  0.0454, -0.6391, -1.8672,  0.6086, -0.6256,  0.8159, -1.2496,\n",
      "         -0.8402, -1.1662,  2.7419, -0.8626,  1.3446, -1.8557,  0.2555,  1.5807,\n",
      "         -0.8134, -0.9551, -1.5794, -1.8157,  1.5301, -1.4135, -0.4456, -0.8424,\n",
      "          0.9663,  0.0538, -1.5965, -1.7118,  0.8216,  2.2135,  0.2181, -0.7249,\n",
      "          0.6337,  0.3060,  2.4481,  0.7957, -2.2571,  1.7426,  0.7518,  2.5588,\n",
      "          0.3605,  0.2382,  0.9165,  1.1561,  0.0592,  0.0852, -3.1272, -1.5849],\n",
      "        [-2.2071,  0.3996,  2.5439,  0.0165, -1.7567, -1.4408,  0.7840,  0.9452,\n",
      "         -1.6737, -1.2458,  0.3420,  0.5990,  0.4563, -2.4572,  0.1409,  1.1976,\n",
      "          2.0539,  0.8490, -0.4046, -0.1258,  1.0922, -0.0835,  1.0964, -0.6058,\n",
      "         -0.6084,  1.5896, -0.1303, -0.1760, -0.1585, -1.7712,  2.0001, -1.1252,\n",
      "          0.3986, -0.3373, -0.1778,  0.5467,  0.2682, -0.0878,  2.3811, -1.6552,\n",
      "         -1.8807, -0.8919,  2.0184,  0.2141, -1.4197, -1.7058, -0.3446, -0.6886,\n",
      "          2.1675,  0.5360, -2.9155,  1.4698,  0.1327, -1.1086, -2.7736,  0.2878,\n",
      "         -0.1584, -0.7231,  1.7642,  0.4261,  1.5665, -1.1526,  2.4705,  0.5894,\n",
      "         -0.3344, -0.3484,  0.7802,  0.6880,  1.0666, -1.9429, -1.1394,  3.2015,\n",
      "         -1.1595,  0.3946,  0.4305,  0.5851, -0.4886,  1.2316, -0.8091, -0.5557,\n",
      "          1.2102,  1.1299,  0.9386, -0.7356, -0.6788, -0.7780,  1.4530, -1.2951,\n",
      "          1.9818,  0.3565, -0.9663, -0.9464, -0.8874,  1.2613, -0.7002, -0.4634,\n",
      "          0.0629,  0.2232,  1.5468,  0.3711,  2.0030,  0.8442,  0.3566,  0.2267,\n",
      "          1.6987,  1.2690, -2.2269, -3.0458,  0.4215,  1.7313, -0.1811,  0.4244,\n",
      "         -0.4334, -0.5803, -0.1558,  0.2169,  0.6433, -1.5644, -0.8113, -2.6462,\n",
      "         -4.2973, -0.7942,  0.5222,  1.2436, -0.7390,  1.6474,  0.5913, -0.1062,\n",
      "          1.2031, -0.0497,  0.6177,  2.2086, -1.9235, -1.0945,  0.4109, -1.0124,\n",
      "         -1.4265,  1.0111, -0.6632,  0.1246, -1.0842, -1.7413,  1.8300,  2.0742,\n",
      "          0.1854,  1.5697, -0.6398, -0.2570,  2.5513, -1.6643, -2.2194,  2.1767,\n",
      "          0.5770,  0.8826, -1.2543,  3.3159,  0.9286,  0.3401, -1.9444, -1.2438,\n",
      "          0.1403,  2.9119,  1.5603,  1.0041, -0.7940, -0.4264,  1.4307, -0.3661,\n",
      "         -0.1408,  1.1232, -0.9220,  2.0510,  0.2441,  0.1441, -0.8899, -0.3884,\n",
      "         -1.1081,  0.3796, -1.6611, -0.4975,  0.9028, -0.3793, -2.1230,  1.7233,\n",
      "          1.1491,  1.8381, -0.1276, -0.5506,  1.9022, -0.8074, -0.7385, -0.4844,\n",
      "         -1.1592, -1.3967, -0.1325,  0.2027, -0.8594,  0.2639, -2.2602,  1.8508,\n",
      "         -0.2133, -0.2329, -2.6417, -0.1733,  1.2263,  1.0406, -1.0850,  0.6321,\n",
      "         -0.1211, -0.9953, -0.7337, -1.4925,  0.4646,  1.5566,  0.9375, -0.9978,\n",
      "          0.4407, -0.3877,  1.9274,  0.9547,  1.0424, -1.6345, -0.8823,  0.2048,\n",
      "         -0.1532,  0.5478, -1.3781, -1.3997,  0.9459, -1.6794, -0.2210, -0.1860,\n",
      "          1.3499,  0.2578, -1.5932, -3.3907,  0.9961,  1.0839, -0.3438, -0.9135,\n",
      "          0.1724,  0.8975,  3.2153, -0.4255, -1.8605,  1.8480, -0.8373,  2.8388,\n",
      "          1.1463, -0.8250,  0.8935, -1.2161, -0.9631,  0.1710, -2.1118, -2.6569]],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "AFTER TCN EXTRACTORS: tensor([[[-1.1436e+00,  2.6387e-01, -2.0640e+00,  ..., -1.4756e+00,\n",
      "          -1.8619e-01,  1.7199e+00],\n",
      "         [-3.7595e+00, -5.0468e+00, -4.1031e+00,  ..., -4.2592e-01,\n",
      "          -2.3448e+00, -3.9658e+00],\n",
      "         [ 2.0418e+00,  2.2555e+00,  3.5879e+00,  ...,  1.9603e+00,\n",
      "           3.2387e+00,  4.3107e+00],\n",
      "         ...,\n",
      "         [-2.2554e-01,  1.7415e+00, -1.4663e+00,  ...,  6.2010e-01,\n",
      "          -7.9978e-01,  1.2189e+00],\n",
      "         [ 1.9610e+00, -8.8818e-01, -8.0590e-01,  ..., -1.9952e+00,\n",
      "          -8.1573e-01,  1.3343e+00],\n",
      "         [ 1.1407e+00,  1.4519e+00,  1.1038e+00,  ..., -1.0364e+00,\n",
      "          -1.7801e+00,  1.4592e+00]],\n",
      "\n",
      "        [[-2.2497e-01, -2.7190e+00,  3.2728e-03,  ...,  1.3200e+00,\n",
      "           1.5765e+00,  5.0480e-01],\n",
      "         [-4.3496e+00, -4.2656e+00, -4.1204e-01,  ..., -2.9130e+00,\n",
      "          -3.8694e+00, -2.8562e+00],\n",
      "         [ 1.3338e+00, -5.6650e-01,  7.3981e-01,  ...,  2.2241e+00,\n",
      "           2.4202e+00,  2.8852e-01],\n",
      "         ...,\n",
      "         [ 1.1818e+00,  4.7154e-01, -8.5580e-01,  ...,  3.2970e-01,\n",
      "          -1.8841e+00, -1.6325e+00],\n",
      "         [ 1.2291e+00, -1.2223e+00,  3.5114e+00,  ..., -1.4139e+00,\n",
      "           1.3097e+00, -1.1286e+00],\n",
      "         [ 2.1418e+00, -3.2246e-01, -6.1165e-01,  ..., -2.2630e+00,\n",
      "          -6.2394e-02, -2.3760e+00]],\n",
      "\n",
      "        [[-1.8559e-01,  1.2354e+00,  2.4040e+00,  ...,  2.0186e+00,\n",
      "           1.9075e+00,  2.4428e+00],\n",
      "         [-2.9471e+00, -1.5597e+00, -3.7071e+00,  ..., -4.4969e+00,\n",
      "          -1.0018e+00, -1.1801e+00],\n",
      "         [-3.6605e-01,  1.9763e+00,  3.6998e+00,  ...,  3.2050e+00,\n",
      "           3.5257e-01,  2.7771e+00],\n",
      "         ...,\n",
      "         [-1.1561e+00, -4.7117e-01, -7.4575e-01,  ..., -1.5067e+00,\n",
      "           6.4156e-01, -2.0707e-01],\n",
      "         [-6.9102e-01, -1.4960e+00, -1.4740e+00,  ..., -2.4519e+00,\n",
      "          -1.5768e-01, -1.2966e+00],\n",
      "         [ 9.0060e-01,  2.2766e+00, -2.0929e+00,  ..., -4.1596e-02,\n",
      "           2.1492e-02,  1.0245e-01]]], grad_fn=<AddBackward0>)\n",
      "MASKS: tensor([[[0.0000, 0.1498, 0.5162,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.8948, 1.6084,  ..., 0.5954, 1.3276, 2.2762],\n",
      "         [1.3835, 0.0000, 0.7912,  ..., 1.6320, 1.7750, 0.5694],\n",
      "         ...,\n",
      "         [1.6486, 2.2063, 1.8735,  ..., 0.0000, 0.2191, 1.5577],\n",
      "         [1.7207, 1.7233, 1.6375,  ..., 0.0000, 0.4439, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.4695, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.3758, 1.2768, 0.5430,  ..., 1.7160, 0.0099, 1.3389],\n",
      "         [1.7798, 2.6007, 2.0108,  ..., 2.2568, 2.9609, 0.9647],\n",
      "         ...,\n",
      "         [1.7544, 1.5410, 1.3784,  ..., 0.0190, 0.9562, 1.1912],\n",
      "         [2.6121, 1.7846, 2.2449,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 1.2411, 0.0000, 0.0092]],\n",
      "\n",
      "        [[0.1455, 0.0000, 0.9251,  ..., 0.0000, 0.1746, 0.0000],\n",
      "         [1.1879, 0.7597, 1.2643,  ..., 0.3992, 1.2280, 0.8147],\n",
      "         [2.1153, 0.9653, 1.1399,  ..., 0.2044, 2.1469, 1.4265],\n",
      "         ...,\n",
      "         [1.4325, 0.4139, 1.6059,  ..., 0.3314, 1.2272, 0.0000],\n",
      "         [0.7492, 1.0083, 0.9140,  ..., 0.0000, 0.7400, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0792,  ..., 0.0000, 0.1039, 0.0000]]],\n",
      "       grad_fn=<ReluBackward0>) tensor([[[1.5026e+00, 9.0930e-01, 1.0013e+00,  ..., 7.2788e-01,\n",
      "          1.0336e+00, 1.1924e+00],\n",
      "         [1.0809e+00, 1.3556e+00, 2.1294e+00,  ..., 0.0000e+00,\n",
      "          1.7330e+00, 2.3237e+00],\n",
      "         [3.0277e+00, 3.8482e+00, 3.4790e+00,  ..., 2.3510e+00,\n",
      "          1.6142e+00, 2.5508e+00],\n",
      "         ...,\n",
      "         [7.7771e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.5081e+00, 1.3634e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[9.1925e-01, 3.8444e-03, 4.2073e-01,  ..., 9.8915e-01,\n",
      "          1.3981e+00, 1.0987e-02],\n",
      "         [1.0243e+00, 2.3796e+00, 1.3551e+00,  ..., 2.7722e+00,\n",
      "          1.4741e+00, 0.0000e+00],\n",
      "         [1.3779e+00, 0.0000e+00, 2.1354e+00,  ..., 4.1562e-01,\n",
      "          9.9685e-01, 1.2051e+00],\n",
      "         ...,\n",
      "         [1.5025e+00, 0.0000e+00, 2.2236e-01,  ..., 0.0000e+00,\n",
      "          1.4767e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[8.8567e-01, 9.2182e-01, 2.1439e+00,  ..., 1.4887e+00,\n",
      "          1.7788e+00, 0.0000e+00],\n",
      "         [1.4418e+00, 1.5350e+00, 2.8152e-01,  ..., 3.5788e+00,\n",
      "          1.7646e+00, 9.6020e-01],\n",
      "         [1.6592e+00, 1.7625e+00, 3.2663e+00,  ..., 2.1430e+00,\n",
      "          1.3967e+00, 2.5463e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]], grad_fn=<ReluBackward0>) tensor([[[0.1072, 0.0118, 0.0000,  ..., 0.0391, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [4.0903, 2.6692, 0.8400,  ..., 1.5766, 0.9748, 0.0000],\n",
      "         ...,\n",
      "         [0.4541, 2.3089, 0.0000,  ..., 0.8328, 0.0000, 0.0657],\n",
      "         [0.0000, 0.5647, 0.3212,  ..., 0.4351, 0.0000, 0.8802],\n",
      "         [0.0000, 1.4620, 0.9925,  ..., 0.2896, 0.1003, 0.7330]],\n",
      "\n",
      "        [[1.0795, 0.2816, 0.0000,  ..., 0.0000, 0.4753, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0769, 0.0000],\n",
      "         [3.4306, 0.2070, 0.0000,  ..., 0.0000, 0.7975, 1.1634],\n",
      "         ...,\n",
      "         [0.0000, 0.1404, 1.8749,  ..., 1.9415, 0.4214, 1.6662],\n",
      "         [0.0982, 1.1805, 0.6992,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0599, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.8453]],\n",
      "\n",
      "        [[0.0473, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.4895,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [2.2774, 1.7091, 1.8158,  ..., 0.8311, 1.8974, 1.1639],\n",
      "         ...,\n",
      "         [0.0000, 0.1890, 0.0000,  ..., 1.3201, 1.4591, 0.9397],\n",
      "         [0.4038, 1.1208, 0.0000,  ..., 0.5656, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.5509,  ..., 0.0563, 0.8999, 0.3239]]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "AFTER MASK MULT: tensor([[[0.0000, 0.0600, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 1.1649, 0.9106,  ..., 0.0000, 0.0000, 0.9693],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.8996, 2.6608, 0.4249],\n",
      "         ...,\n",
      "         [1.3722, 0.0000, 0.0000,  ..., 0.0000, 0.2612, 0.8189],\n",
      "         [0.4310, 1.2351, 0.6690,  ..., 0.0000, 0.0112, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.5681, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0786, 0.1414, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.8293, 0.0000, 1.0211,  ..., 3.2188, 0.0000, 0.5498],\n",
      "         ...,\n",
      "         [1.1148, 0.6105, 0.0000,  ..., 0.0089, 0.0000, 1.8722],\n",
      "         [1.4135, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0094]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.3503,  ..., 0.0000, 0.0244, 0.0000],\n",
      "         [0.4311, 0.0000, 0.0000,  ..., 0.2139, 0.0000, 0.0000],\n",
      "         [2.2651, 0.1770, 0.6275,  ..., 0.1366, 1.0854, 0.5172],\n",
      "         ...,\n",
      "         [0.3841, 0.1745, 0.4253,  ..., 0.0000, 1.5515, 0.0000],\n",
      "         [0.2695, 0.0000, 0.2031,  ..., 0.0000, 0.3191, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0249,  ..., 0.0000, 0.0431, 0.0000]]],\n",
      "       grad_fn=<MulBackward0>) tensor([[[7.0464e-01, 0.0000e+00, 0.0000e+00,  ..., 4.5564e-02,\n",
      "          6.3945e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 3.9478e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 1.1003e-02],\n",
      "         [8.3355e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.9699e-01, 1.0935e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          2.0497e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[3.1641e-01, 8.1452e-05, 1.4423e-02,  ..., 0.0000e+00,\n",
      "          1.4149e-01, 5.8934e-03],\n",
      "         [0.0000e+00, 9.3348e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [1.3346e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 6.1865e-01],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          5.2078e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[1.4527e-02, 5.9718e-01, 7.4374e-01,  ..., 4.8828e-01,\n",
      "          2.0157e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 1.1802e+00, 3.2062e-02,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          9.3504e-01, 9.7915e-01],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]], grad_fn=<MulBackward0>) tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.1378e-03,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 2.0921e-01,  ..., 3.3851e-01,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 2.1278e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 4.1278e-01],\n",
      "         [0.0000e+00, 3.6577e-01, 4.5555e-01,  ..., 1.6799e-01,\n",
      "          2.1620e-02, 7.2310e-02]],\n",
      "\n",
      "        [[0.0000e+00, 4.7534e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          1.5497e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [1.5488e+00, 6.6394e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 1.6062e-01],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 1.0108e-01,  ..., 0.0000e+00,\n",
      "          6.9790e-02, 0.0000e+00],\n",
      "         [0.0000e+00, 1.0816e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 3.6320e-02]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0965e-01,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [1.9561e-01, 0.0000e+00, 7.4247e-01,  ..., 3.2879e-01,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 1.1617e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2517e-03,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 2.8495e-01,  ..., 3.3764e-02,\n",
      "          0.0000e+00, 0.0000e+00]]], grad_fn=<MulBackward0>)\n",
      "AFTER DECODER: tensor([[[-0.4585, -1.1885,  0.0104, -1.2912, -1.0595,  0.0926, -0.6353,\n",
      "          -1.0869, -0.6964, -0.2887, -1.7304, -0.6631, -0.4627, -1.5273,\n",
      "          -2.6025, -1.1233, -1.6908, -0.7376, -0.7445,  0.9560,  0.3855,\n",
      "          -1.0651,  0.4460, -2.0560, -0.9205, -0.6744,  0.7356, -1.2883,\n",
      "           0.6099,  1.7034,  0.3656,  0.0092,  0.0779, -0.2235, -0.0043,\n",
      "          -1.6743, -1.0866, -1.9665, -0.0917,  0.8851,  0.0476,  0.0913,\n",
      "          -0.9917,  0.6617, -0.8885, -2.0563,  0.4400, -0.4015, -0.4842,\n",
      "           1.2041, -0.9015,  0.4637, -0.6375, -1.5772,  0.5076,  0.1192,\n",
      "          -1.6122, -2.8793,  0.9394,  0.3664,  1.0680,  0.5488,  0.2087,\n",
      "           0.9533, -0.5025, -2.7984, -0.6249, -0.2522, -1.7231, -1.3766,\n",
      "           0.7787, -0.3024, -0.6209,  0.1419,  0.4573, -1.9921, -0.6215,\n",
      "          -0.9426, -1.1623, -0.8342, -0.1187,  0.2486,  2.1757, -1.5293,\n",
      "           1.2288,  0.1017, -0.7160, -1.5583,  1.2905, -0.0226,  0.1934,\n",
      "           1.3589, -0.4895, -0.2338,  0.4880, -2.3083,  0.0941, -1.3580,\n",
      "           0.4796, -0.6307]],\n",
      "\n",
      "        [[-1.2310, -0.2474,  0.5076, -0.7905, -0.2773,  0.4538, -0.8937,\n",
      "          -0.1722,  0.1815, -0.2070, -1.3210, -0.0617, -0.8725, -0.4930,\n",
      "          -0.2205, -0.7524, -0.6748, -0.3670, -0.5917,  0.4324, -0.9933,\n",
      "          -1.5494,  0.9831,  1.1102,  0.1541,  0.5927,  1.2394, -2.7663,\n",
      "          -0.8400, -0.5317,  0.2075,  1.2535, -0.5098, -1.2699, -1.5732,\n",
      "          -0.7944, -1.1005,  1.1583, -1.4323, -0.4999, -1.7793, -2.7222,\n",
      "          -0.9859, -1.3882,  0.4777, -0.1461, -2.0610, -0.9255, -3.2916,\n",
      "          -1.2765, -2.0452, -0.1701, -2.1094, -1.2970,  1.5732, -0.8828,\n",
      "          -1.1668, -0.8414, -3.1735, -0.3396,  0.0633, -0.0347, -1.8275,\n",
      "          -3.2753,  0.7007, -1.4303, -1.7601, -1.6075, -0.4321, -2.6145,\n",
      "           0.6838,  0.4127, -0.2934,  0.3255,  0.4170, -2.1642, -2.1828,\n",
      "          -1.3238, -0.5788, -1.8808, -0.6515,  0.8338,  0.5459,  0.3116,\n",
      "          -1.5988, -0.7468, -2.6253, -0.0490, -0.0631, -2.1762, -1.1900,\n",
      "           0.4038,  0.7761, -0.8909,  0.0950, -1.3884, -0.6524, -1.3606,\n",
      "          -2.7560, -2.1374]],\n",
      "\n",
      "        [[-0.6076,  0.8593, -0.0968, -1.0520, -0.4615,  0.1345, -0.3269,\n",
      "           0.5038, -1.4035, -0.7359,  0.2968, -0.8865,  0.1326, -0.9516,\n",
      "          -0.2844, -0.5663, -0.9829, -0.4819, -0.5343, -0.2998,  1.0440,\n",
      "           0.1442, -1.1866,  0.2405, -0.3094, -0.1149,  0.7730, -1.5150,\n",
      "          -0.2579,  1.5744, -0.3785, -1.5112,  0.8410, -1.7042,  0.3423,\n",
      "          -1.6273,  1.8654, -3.4064,  2.0989,  0.8171, -0.7412,  0.0954,\n",
      "           0.1871, -0.6130, -1.1102, -1.8328, -2.5656, -1.3445,  1.7704,\n",
      "          -0.9964, -0.4310, -0.2812,  0.6886, -1.1818, -0.4482, -1.0199,\n",
      "          -0.3455, -1.1983, -1.1491, -0.9275,  1.4845,  1.2295, -1.7258,\n",
      "           0.4814, -0.7954,  0.3394, -1.4311,  0.2619, -1.9198,  1.8587,\n",
      "          -0.2922, -1.0661, -0.7313, -1.8476,  0.0175, -0.2374, -1.7662,\n",
      "          -0.9360, -1.0891, -1.5907, -0.6259, -1.3806, -0.4158, -1.0383,\n",
      "          -0.0649, -0.1724,  0.1982, -2.3370, -1.1873, -1.1313, -0.8070,\n",
      "           0.4694,  1.3583, -0.6278, -0.5896, -1.4825, -0.2014, -0.0950,\n",
      "          -1.7849, -1.3056]]], grad_fn=<ConvolutionBackward0>) tensor([[[-0.1023,  0.4203, -0.4096, -0.0242,  0.6084,  0.2572, -0.1219,\n",
      "           0.0827,  0.1762, -0.8402, -0.9818, -0.3921, -0.1051,  0.3334,\n",
      "          -0.0808, -0.4855, -0.4815,  0.0169, -0.0652,  0.2895, -0.4632,\n",
      "           0.8042,  0.2458,  1.5190, -0.3027, -0.2981,  0.1806,  0.0050,\n",
      "           0.3426,  0.4649, -0.7024, -0.4016, -0.2941, -0.4694,  0.3262,\n",
      "          -0.8839, -0.2176, -0.6793, -0.2489, -0.8950, -0.2422,  0.0968,\n",
      "          -1.0568,  0.4764,  0.7422, -0.0265, -0.3826, -0.5970,  0.8872,\n",
      "          -0.7501, -1.1851, -1.6615, -0.2850, -1.0682, -0.1344, -0.8123,\n",
      "           0.8968, -0.1376,  0.5799, -1.3885, -1.1917, -0.0648,  0.3244,\n",
      "           0.0175,  0.0180,  0.0967,  0.4662, -1.5414,  0.3884, -0.6445,\n",
      "          -1.9562, -1.1492,  0.6181,  0.9808, -0.8714,  0.6210, -0.0971,\n",
      "           0.9742,  0.6636, -1.7387,  0.1111,  0.4268,  0.1470,  0.7169,\n",
      "          -0.7006, -0.4249, -1.0722, -0.4078,  0.9045, -0.3856, -0.4950,\n",
      "          -1.1626,  0.3258, -1.5936,  0.1184, -0.9787,  1.1926,  0.6041,\n",
      "           1.5827, -1.0194, -0.1275, -0.6877, -0.8961, -0.4847,  1.0023,\n",
      "          -0.6825, -0.0745, -0.5363,  0.6294, -0.3813,  0.0212, -0.6346,\n",
      "          -0.0727,  0.7315,  1.3831,  0.1271, -0.9777, -0.9507,  0.8314,\n",
      "           0.0255, -0.1963,  0.3464, -0.9989, -0.6981,  0.1829, -0.7535,\n",
      "          -0.0212,  0.2361, -0.6278, -0.2050, -0.7008, -0.3500,  0.5054,\n",
      "          -0.3740,  0.8284,  0.2720,  0.2161,  0.2541,  0.2824,  0.0923,\n",
      "          -0.1765, -0.0329,  0.0141, -0.2429,  0.2495,  0.2210, -0.1391,\n",
      "          -0.0432, -0.3054, -0.2079, -0.3616, -0.1996,  0.1899,  0.2591,\n",
      "          -0.0662, -0.0285,  0.0355,  0.1975,  0.2248,  0.4095]],\n",
      "\n",
      "        [[ 0.1571, -0.2540, -0.5092, -0.0617, -0.1895,  0.2263,  0.3451,\n",
      "          -0.1875,  0.4725, -0.7962, -0.3537,  0.0179,  0.6193,  1.1345,\n",
      "           0.8213,  0.3190, -0.2072, -0.7388, -0.4762, -0.0239, -0.2612,\n",
      "           1.6194,  0.5358,  0.1297,  0.1212, -0.7212,  0.9660, -1.1486,\n",
      "          -0.7126, -0.0689,  1.0449, -0.3610,  0.8174,  1.1041,  1.3201,\n",
      "          -0.7854, -0.4150, -0.5251, -0.1615, -0.1644,  0.0298,  0.3716,\n",
      "          -0.7674,  1.1143,  0.2880, -0.5989, -0.1117, -0.2719, -0.3512,\n",
      "          -0.8800, -0.9560,  2.2226,  1.4470, -1.0947,  1.8657, -0.9649,\n",
      "           0.4062,  0.3658, -0.9524, -1.3921, -0.9431, -0.5435,  1.3925,\n",
      "           0.0656,  0.8168, -1.7498,  1.0902, -1.5238,  0.7216, -0.2464,\n",
      "          -1.7052,  0.5657,  0.3280,  2.0786, -0.1558, -0.5550,  0.6488,\n",
      "          -2.0020, -0.2133,  1.2107, -0.7511, -0.3652, -1.2034, -0.5420,\n",
      "           0.2379,  0.2413,  0.9104, -0.8225,  0.5450,  0.2579,  1.6101,\n",
      "          -0.9543,  0.6750, -0.5485,  1.3255,  0.1273, -0.2012, -0.3787,\n",
      "           0.6014,  0.2645, -1.3800, -0.9637, -0.5069, -0.4569,  1.5205,\n",
      "           0.5314, -0.3671,  0.1621,  0.8363,  0.2244,  0.8757, -0.6237,\n",
      "          -1.1296, -0.6387,  0.1742,  0.6521,  0.5145, -0.3882, -0.9113,\n",
      "           0.6217, -0.9075,  0.1699,  0.7852, -0.3503,  0.7406,  0.2503,\n",
      "           0.4135, -0.1457,  0.5148, -0.1476, -0.7020, -0.0663, -0.4377,\n",
      "          -0.0240,  1.2281, -0.0345,  0.6289,  0.1236, -0.3230,  0.1998,\n",
      "          -0.4376, -0.4540,  0.2058, -0.4667,  0.2217,  0.4953,  0.5394,\n",
      "           0.1257,  0.0175,  0.6574, -0.0851, -0.1685,  0.0820,  0.1130,\n",
      "          -0.0294, -0.0352, -0.1793,  0.0760,  0.0856,  0.1891]],\n",
      "\n",
      "        [[ 0.1704, -0.2037, -0.0758,  0.3041, -0.4231,  0.3342, -0.0514,\n",
      "          -0.1613, -0.1140, -0.5394, -0.3288,  0.9514,  1.0542,  1.1932,\n",
      "          -0.4002,  0.3094,  0.0330, -0.9782,  0.0584,  0.1677, -0.3992,\n",
      "           1.0321,  0.7364, -0.1895,  0.6617, -0.7258, -1.0957, -1.1010,\n",
      "           0.3372, -0.2008, -0.8165,  0.3969, -0.1869,  0.4726,  1.0033,\n",
      "           0.6426,  0.1007, -2.4758,  0.3947, -1.2594, -1.3217,  1.0743,\n",
      "          -0.4188, -1.3893,  1.0615,  0.6112,  0.0315,  0.2841,  0.0195,\n",
      "          -0.8010,  1.3069,  0.1607,  0.8521, -0.2705,  0.8283, -0.6220,\n",
      "           0.1570, -2.2005,  1.2578, -0.0387, -1.1752,  0.0947, -0.5009,\n",
      "          -1.1430,  1.2284, -0.6200, -1.0909,  0.2915,  0.1336, -1.1295,\n",
      "           0.6803,  0.0360, -1.4624, -0.9931, -1.2907, -0.4553, -0.6012,\n",
      "          -0.1570, -0.3656,  0.0131, -0.9483,  0.4433,  0.9131, -1.1276,\n",
      "           1.7500,  0.7846,  0.8246, -0.4681,  0.8831, -0.3300, -0.0710,\n",
      "          -0.5711, -0.1963, -0.2976,  1.7550,  0.4571,  0.5695,  0.2693,\n",
      "           1.1394,  0.3329, -1.9314, -1.2283, -0.3359, -0.7616,  0.3666,\n",
      "           0.6293,  1.0271,  0.2899, -0.0746,  0.6747, -0.1706,  0.4605,\n",
      "          -0.5165, -0.7367,  0.0049,  0.8677, -0.2200,  0.1935,  0.1826,\n",
      "          -0.2316, -0.2360, -0.4927, -0.1622, -0.2535,  0.9431,  0.0362,\n",
      "           0.5848, -0.1494,  0.1621,  0.2671, -0.3356, -0.4001, -0.0793,\n",
      "           0.0388,  0.1132, -0.0442,  0.0065, -0.3235,  0.0496,  0.0549,\n",
      "          -0.4856, -0.4066,  0.2270,  0.3449,  0.4361, -0.1729,  0.1038,\n",
      "           0.1267,  0.3325,  0.5134, -0.2704,  0.3049,  0.0385,  0.0191,\n",
      "          -0.0652, -0.0078, -0.0118,  0.2324,  0.1419,  0.3067]]],\n",
      "       grad_fn=<ConvolutionBackward0>) tensor([[[-3.3017e-02,  2.8473e-01,  1.3709e-01, -6.9569e-02,  2.1460e-01,\n",
      "           7.3989e-02, -7.8786e-05,  1.1335e-01,  2.6510e-01,  1.6002e-01,\n",
      "          -2.7827e-01, -1.3907e-01, -5.1491e-02,  3.3833e-01,  4.3064e-01,\n",
      "          -1.9072e-01, -8.8915e-02, -1.7259e-02,  3.3717e-01,  3.0159e-01,\n",
      "           5.6502e-01, -1.9224e-01,  2.7086e-02,  3.6783e-01, -7.2256e-02,\n",
      "          -1.1335e-01, -4.8336e-01,  2.8288e-01,  4.2647e-01,  2.9944e-01,\n",
      "          -6.1926e-02, -2.3946e-01, -4.1800e-01, -1.3715e-01,  4.1114e-01,\n",
      "          -2.6905e-01,  2.3368e-01,  1.1910e-01, -2.4712e-01,  1.6853e-02,\n",
      "          -3.8742e-02, -3.6426e-01,  3.0557e-01,  3.5065e-01, -3.6614e-01,\n",
      "           1.1872e-02, -4.5534e-01, -7.7437e-01, -3.9202e-01, -2.5896e-02,\n",
      "          -3.4482e-01, -2.5397e-01,  3.1745e-01,  2.8763e-01, -4.2724e-01,\n",
      "          -4.1702e-01,  2.3683e-01,  2.1767e-01,  3.5055e-01,  3.0073e-02,\n",
      "          -3.6149e-01,  5.1600e-01, -4.4724e-02, -1.4235e-01,  1.3333e-01,\n",
      "          -3.9636e-01,  3.8429e-01, -2.7468e-01,  3.8114e-01,  2.1853e-01,\n",
      "           3.6923e-01,  2.4802e-01, -3.8403e-01, -4.9725e-01, -5.2236e-03,\n",
      "           1.6096e-01, -2.7984e-01, -3.4039e-01,  4.9366e-01, -1.3029e-01,\n",
      "          -5.0573e-02,  9.2216e-02,  4.6975e-01, -2.1039e-01,  4.3929e-01,\n",
      "           2.8992e-01, -8.1248e-02,  4.4937e-01,  4.0895e-01,  1.7341e-01,\n",
      "           2.8387e-01, -8.8725e-02,  4.2729e-01,  6.3272e-01,  1.2251e-01,\n",
      "           3.9952e-01, -9.6479e-01, -2.8486e-01, -2.4176e-01, -1.1745e+00,\n",
      "          -5.3519e-01,  3.4610e-01,  3.5824e-01, -7.7413e-01,  1.4056e-01,\n",
      "           2.2028e-01, -7.5349e-01,  3.9042e-01, -3.8387e-03, -3.8587e-01,\n",
      "           3.7528e-01, -1.8823e-01, -3.2650e-01, -3.1021e-01, -1.0282e-01,\n",
      "          -3.3910e-01, -4.1822e-01, -2.0762e-01, -2.4345e-01, -5.3127e-01,\n",
      "           4.0822e-01, -4.5364e-02,  5.5385e-02, -3.4402e-01, -7.1673e-01,\n",
      "          -1.1445e+00,  2.7256e-01, -7.4373e-01, -1.2232e-01, -1.0579e-01,\n",
      "           2.6176e-01,  2.3243e-01,  8.6679e-01, -5.9261e-01, -1.3444e-01,\n",
      "          -5.3491e-01,  1.9831e-02,  7.2924e-01, -1.0057e+00, -8.2411e-01,\n",
      "           9.8473e-01,  4.0358e-01,  2.0924e-01, -8.0258e-01,  6.1936e-01,\n",
      "          -1.1964e-01,  2.4733e-01,  5.1498e-01,  9.1630e-01, -1.6220e-01,\n",
      "           3.3132e-01, -3.3327e-02,  1.2554e-01, -7.2817e-02,  4.8846e-01,\n",
      "           8.8009e-02,  5.8610e-01,  9.3258e-02, -4.0982e-01,  7.3120e-01,\n",
      "          -2.8271e-01,  5.5729e-01, -1.6365e-01,  4.5550e-01,  3.2211e-01,\n",
      "           2.7381e-01,  5.2865e-01, -6.0761e-01, -3.4160e-01,  7.6440e-01,\n",
      "          -4.8241e-02,  2.5253e-01, -9.4782e-02,  4.5062e-01,  5.9461e-01,\n",
      "           4.7376e-01,  5.5529e-01, -7.0137e-01, -3.1831e-01, -2.7239e-01,\n",
      "           2.9811e-01,  4.3555e-01, -2.3591e-01,  4.0794e-01,  3.2667e-01,\n",
      "           3.9207e-02,  3.3902e-03,  1.6050e-01, -2.2415e-01,  4.9562e-01,\n",
      "           1.4194e-01,  5.5893e-01,  1.4257e-01, -2.2751e-02,  2.8601e-01,\n",
      "           1.3477e-01, -2.0023e-01, -5.6650e-04,  5.0283e-02, -3.1352e-02,\n",
      "           9.4753e-02, -6.8774e-02,  3.1449e-01, -4.0719e-01, -3.5164e-01,\n",
      "           4.1867e-01,  2.3364e-01, -2.9438e-01, -2.8473e-01,  4.2511e-01,\n",
      "           1.3058e-01,  2.8501e-01,  6.7430e-02,  1.9914e-01,  2.0533e-01,\n",
      "          -1.4858e-01,  3.2124e-01,  8.3272e-02, -7.2598e-02, -1.7231e-01,\n",
      "          -5.2869e-02,  1.3149e-01, -5.8994e-02, -4.9997e-02,  3.4302e-02,\n",
      "           8.9590e-02,  2.3380e-01,  1.5677e-02,  1.4719e-01, -1.5144e-02,\n",
      "          -7.1211e-02,  6.1626e-02,  4.0708e-02,  3.0831e-02,  1.6206e-01,\n",
      "          -5.8072e-02,  1.2156e-01, -4.2375e-04,  3.6004e-02,  1.5446e-01]],\n",
      "\n",
      "        [[ 1.9765e-01, -1.4860e-01,  1.1930e-01,  3.3379e-01,  2.4779e-01,\n",
      "          -1.7422e-01,  1.4745e-01, -1.3922e-01, -1.7671e-02,  1.1172e-02,\n",
      "          -4.2357e-01,  3.2503e-01, -1.5513e-01, -1.4009e-01, -1.5078e-01,\n",
      "          -1.1735e-01,  1.7609e-01,  1.9614e-01,  4.9221e-02, -7.3929e-02,\n",
      "          -1.9815e-01,  2.4998e-01, -1.0771e-01,  1.5183e-01,  3.0375e-01,\n",
      "          -7.5563e-01,  8.0616e-02, -7.7252e-02, -5.5158e-02,  6.1707e-01,\n",
      "          -2.1962e-01,  1.4369e-01, -2.0416e-01, -1.3382e-01,  1.9034e-01,\n",
      "           2.7414e-01, -1.6313e-01,  6.2161e-01,  1.9301e-01,  4.2255e-01,\n",
      "           3.1596e-01, -2.2540e-01,  1.4882e-02, -5.4235e-01, -2.8462e-01,\n",
      "          -4.8656e-01,  4.0457e-02, -8.8233e-02, -7.9212e-01, -4.5317e-01,\n",
      "          -1.1084e-01,  5.8083e-02,  5.8355e-02, -2.2285e-01,  6.7828e-02,\n",
      "           7.1105e-01, -6.1717e-01,  3.6326e-01,  3.7637e-01,  4.8362e-01,\n",
      "           1.1576e-01, -1.6517e-01,  3.9507e-01, -6.3618e-01,  6.6780e-01,\n",
      "          -1.6720e-01, -3.8837e-01, -4.7233e-01, -2.1520e-01, -4.6194e-01,\n",
      "           1.2616e-01,  6.9024e-01, -6.2515e-01, -1.9858e-01,  5.3342e-01,\n",
      "          -4.0287e-01, -1.5435e-01, -9.9352e-02,  2.9637e-01,  2.3287e-01,\n",
      "          -5.2162e-01, -3.4163e-01,  1.0289e-01,  2.6096e-01,  5.9944e-01,\n",
      "           3.5609e-01, -6.7808e-01,  7.0806e-01,  4.3832e-01,  3.3919e-02,\n",
      "           4.8406e-01, -3.1317e-01,  4.6630e-01,  3.0726e-01, -4.3127e-01,\n",
      "          -6.4658e-02,  7.0159e-02, -8.4825e-01, -2.9561e-01, -1.0203e+00,\n",
      "          -3.7050e-01, -2.8242e-01, -5.1832e-01, -9.4061e-01,  3.9375e-01,\n",
      "           3.3176e-01, -9.2748e-01, -7.8183e-01,  4.2217e-01, -2.1692e-01,\n",
      "           1.0988e-01,  2.6963e-01, -3.4131e-01,  3.3166e-01,  2.2037e-01,\n",
      "          -7.8355e-01, -3.9384e-01,  4.6450e-01,  2.9066e-01, -1.9813e-01,\n",
      "           3.3294e-01, -2.0154e-01,  3.2666e-01, -4.5581e-01,  3.7910e-01,\n",
      "          -9.6219e-01, -3.5054e-01,  1.7421e-01, -5.6443e-01, -2.7424e-01,\n",
      "          -7.0977e-01,  6.9119e-01,  3.3983e-02, -3.1826e-01,  3.3087e-02,\n",
      "          -3.2917e-02, -1.0017e-01,  9.5313e-01, -6.5469e-01, -2.3203e-01,\n",
      "           6.3612e-02,  5.4632e-01,  4.2863e-01, -5.3166e-02,  3.8119e-01,\n",
      "           3.3508e-01,  2.3101e-01,  2.6521e-01,  2.1604e-01, -9.0460e-01,\n",
      "          -3.8997e-03,  8.2424e-02, -9.3537e-01,  7.7542e-01,  8.5070e-01,\n",
      "           1.6602e-01,  8.6376e-01, -2.8701e-01, -8.5325e-01, -1.0613e-01,\n",
      "           3.5678e-01, -1.3266e-01,  1.2357e+00,  6.7613e-01, -3.3816e-01,\n",
      "          -2.4762e-01,  9.3625e-01, -8.8872e-01, -4.2263e-01,  9.9365e-04,\n",
      "          -1.4179e-01, -1.1822e-01,  3.4662e-01, -6.2427e-01, -2.6764e-01,\n",
      "          -1.1884e-01,  1.9989e-01, -3.8566e-01,  4.3156e-02,  4.2729e-01,\n",
      "           6.5727e-03, -4.3294e-02,  4.0217e-01,  1.1274e-01, -1.5053e-02,\n",
      "           4.6207e-01,  5.3642e-01,  4.1327e-01,  8.7815e-02,  5.7081e-02,\n",
      "           1.1063e-01,  4.2805e-02, -2.6880e-01,  2.2236e-01,  4.9291e-01,\n",
      "           2.8639e-02,  3.7610e-01, -2.1635e-01,  5.6408e-02,  4.3625e-01,\n",
      "           5.4390e-02,  3.6554e-02, -3.2893e-01,  1.2371e-02,  2.8044e-01,\n",
      "           1.7266e-01,  2.9440e-01, -1.2634e-01,  4.1623e-01,  3.6366e-01,\n",
      "          -1.9038e-01,  1.2090e-01,  1.2687e-01, -6.7386e-02,  8.3412e-02,\n",
      "          -3.4062e-01, -1.2449e-01, -9.8908e-02,  8.7584e-02,  3.2143e-01,\n",
      "          -3.8388e-02,  1.0903e-01,  9.8065e-02,  3.0807e-01,  1.8071e-01,\n",
      "          -1.4840e-01, -1.5261e-01, -1.8331e-02,  1.2862e-01, -1.3076e-01,\n",
      "          -1.0752e-01,  2.3597e-02,  2.7109e-02, -1.1582e-02,  1.0656e-01,\n",
      "          -3.7596e-02,  1.3450e-01,  2.4709e-02,  1.5797e-01,  6.5080e-03]],\n",
      "\n",
      "        [[-3.9260e-04, -4.4585e-02,  2.5097e-02, -4.4071e-02,  5.0668e-02,\n",
      "          -1.9850e-01, -9.3132e-02,  7.4639e-02,  3.8375e-02,  1.4230e-01,\n",
      "          -2.6549e-01,  9.4193e-02, -2.4095e-01,  1.5083e-01, -2.5401e-01,\n",
      "          -1.5795e-01,  5.1672e-01,  8.6135e-02,  3.6100e-02,  4.5241e-01,\n",
      "          -6.4678e-02,  1.2037e-02, -1.6484e-01, -2.8532e-01, -3.3821e-01,\n",
      "          -7.5462e-02,  1.2032e-01, -2.1534e-02,  4.6739e-01,  2.1166e-01,\n",
      "           4.9123e-01, -4.6966e-02,  2.5581e-01,  2.0403e-01,  3.2207e-01,\n",
      "          -2.0891e-01,  2.1158e-01, -4.2431e-01, -6.9761e-02,  2.8127e-01,\n",
      "           5.0968e-01,  1.3214e-01, -7.0234e-01, -1.6923e-01,  1.6045e-01,\n",
      "          -9.1572e-02, -6.5940e-01,  2.0999e-01, -2.1740e-01,  1.2601e-01,\n",
      "          -3.8786e-01,  6.2806e-01, -1.0541e-01,  1.2926e-01,  1.0899e+00,\n",
      "           4.6477e-02, -1.4772e-01, -4.8957e-03, -1.0921e-01, -3.5988e-01,\n",
      "          -4.8678e-01, -2.0137e-01, -4.2367e-01,  5.6087e-01,  2.6780e-01,\n",
      "           4.7179e-01,  2.8901e-01,  9.1607e-01,  6.6184e-02,  3.8277e-01,\n",
      "           2.9173e-01, -1.1450e-01,  6.9693e-01, -4.2264e-01, -3.0361e-01,\n",
      "           1.1347e-03, -3.7519e-01,  1.4424e-01, -3.7814e-01,  1.8533e-01,\n",
      "          -1.3673e-02, -2.0474e-01, -1.7336e-01, -4.0657e-01, -6.0628e-02,\n",
      "          -3.5606e-01,  3.6664e-01,  8.9001e-02, -2.5409e-01,  8.5836e-02,\n",
      "          -7.9087e-01, -8.7596e-02, -6.5475e-02, -1.0742e-02,  6.9908e-01,\n",
      "          -6.2453e-01, -7.9925e-01, -2.7677e-01,  3.2296e-02, -5.1721e-01,\n",
      "           2.3413e-01, -1.7373e-01, -3.7648e-01,  4.8037e-02, -7.5372e-02,\n",
      "          -2.6783e-01,  5.0592e-01, -3.4371e-01, -4.3601e-01, -4.9312e-01,\n",
      "           7.3810e-01, -9.2818e-02, -1.9075e-02, -1.6338e-02, -2.8854e-01,\n",
      "           4.7320e-01,  2.5306e-01, -2.5654e-02,  2.4211e-01,  4.8104e-01,\n",
      "          -1.7755e-01,  2.7249e-01,  3.5561e-01, -7.8525e-01,  3.0315e-01,\n",
      "          -1.3266e-01, -7.4672e-02,  3.4075e-01, -4.1573e-01, -3.3946e-01,\n",
      "          -1.5021e-01,  8.3532e-01,  3.6536e-01,  3.7555e-02,  1.6548e-01,\n",
      "           6.4299e-01,  3.7350e-01, -3.8233e-01,  5.4624e-02, -2.6143e-02,\n",
      "           5.3407e-01,  7.9739e-01, -8.1279e-01, -9.7088e-02,  4.8550e-01,\n",
      "           1.3272e-01,  9.2239e-02,  7.3369e-01, -3.5557e-01,  2.0784e-01,\n",
      "           1.2763e-01,  2.1916e-01,  7.6418e-01, -1.2466e-01,  4.3579e-01,\n",
      "           1.6174e-01,  1.9789e-01, -6.8906e-01, -1.0262e+00,  4.4871e-01,\n",
      "           1.8399e-01,  2.7758e-01, -6.4013e-01,  4.0715e-01,  3.9138e-01,\n",
      "          -2.1399e-01, -4.5253e-01,  5.9643e-01,  5.3732e-01,  4.9231e-01,\n",
      "          -1.4297e-01,  7.5837e-01, -1.6566e-01, -3.9500e-02,  6.7218e-01,\n",
      "           3.2356e-01,  6.7423e-01, -5.2886e-01,  4.7371e-02, -1.8943e-01,\n",
      "           1.5022e-01,  1.4952e-01, -6.8181e-01, -7.2099e-01,  1.7776e-01,\n",
      "           3.0968e-01,  6.1236e-01,  4.8218e-02, -8.2479e-04,  7.6750e-01,\n",
      "           1.7540e-01,  1.2251e-01,  5.0814e-02,  2.7295e-01,  6.0494e-01,\n",
      "           2.7424e-01,  3.6342e-01, -2.0980e-01, -6.5763e-03,  2.7891e-01,\n",
      "           6.0045e-02,  1.8580e-01, -1.0689e-01,  2.4854e-01,  3.4859e-01,\n",
      "           1.5109e-01,  4.8757e-02, -7.9470e-02,  1.7942e-01,  2.4011e-01,\n",
      "          -2.6889e-02, -5.8552e-02, -1.9148e-01,  8.9724e-02,  2.5258e-01,\n",
      "           1.4645e-01,  2.3096e-01,  2.4410e-02, -1.9861e-01,  1.4187e-01,\n",
      "           1.1002e-03,  1.7302e-02, -2.4292e-01,  1.5099e-01, -9.0133e-02,\n",
      "           2.2683e-01,  2.0757e-01, -9.4347e-02,  1.7603e-01,  1.7178e-01,\n",
      "          -1.1316e-01, -7.4006e-02,  5.6499e-02, -7.2769e-02, -3.0625e-02,\n",
      "           9.6584e-03,  6.1600e-02, -9.4922e-02,  1.3680e-01,  8.4665e-02]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "AFTER LOGITS: tensor([[ 0.2196,  0.6280, -0.1244,  0.3272,  0.3680,  0.1338, -0.2646, -0.2493,\n",
      "          0.4093, -0.2704,  0.2877,  0.5756,  0.0218,  0.9238, -0.0052,  0.0331,\n",
      "         -0.7325, -0.8172,  0.5568, -0.2210,  0.2688, -0.6325,  1.1442,  1.0010,\n",
      "          0.6624, -0.5016,  1.5650, -0.0370,  0.4649, -1.1103, -0.1925,  0.2017,\n",
      "         -1.1377,  0.2535, -0.5457, -1.0257,  0.3150,  0.9444,  0.3574, -0.7354,\n",
      "         -0.6903,  1.3814, -0.1941, -0.4234,  1.4954, -0.1263, -0.7658,  1.0938,\n",
      "         -0.5260, -0.8605,  0.8115, -0.1932, -0.3877, -0.0727, -0.8843, -0.6848,\n",
      "         -0.2658, -0.0189, -0.4729,  0.0846, -0.2850,  0.1005, -1.9222, -0.8244,\n",
      "          0.5151, -0.4045, -0.5945, -0.4554,  0.6730, -0.1270,  0.2954, -0.3833,\n",
      "          0.9348, -0.3202,  0.7202,  0.0641, -0.1265, -0.1459, -1.4276,  0.0633,\n",
      "          0.3319,  0.5117, -0.5099,  0.1260, -0.3541,  0.0899, -0.6776,  0.0851,\n",
      "          0.1068, -0.1468, -1.3812,  0.2916, -0.1440,  0.4046,  0.5980, -1.2623,\n",
      "          0.4397, -0.3673, -0.2574,  0.3915,  0.4948],\n",
      "        [ 1.2199,  0.6468,  0.3181, -0.1235,  0.3905, -0.3647, -0.5511,  0.1375,\n",
      "          0.1109, -0.2703,  0.9896,  0.4830, -0.0338,  0.8168,  0.1686,  0.0175,\n",
      "         -1.2755, -0.7684,  0.0832,  0.4781,  0.4625, -1.1074,  0.3334,  0.9638,\n",
      "          0.8264,  0.1767,  1.2236, -0.5537,  0.8687, -0.3666, -0.2086, -0.0891,\n",
      "         -0.9317,  0.4383, -0.3622, -1.4414,  0.4243,  0.5940,  0.3876, -0.7680,\n",
      "         -0.8949,  0.8544,  0.1319,  0.0068,  0.9357, -0.0499, -0.7680,  0.0696,\n",
      "         -1.1381, -0.8526,  0.5973, -0.4816, -0.3863, -0.7078, -0.2063, -0.3158,\n",
      "          0.0737, -0.1945, -0.7454,  0.0632,  0.3051, -0.2405, -1.4035, -0.5280,\n",
      "          0.8377, -0.4796, -0.8039, -0.5347,  0.7602, -0.4095,  0.6575, -0.1696,\n",
      "          0.3523, -0.7464,  0.7361, -0.3971,  0.2678, -0.6415, -1.0509, -0.0046,\n",
      "          0.6349,  0.3830, -0.3427, -0.0532, -0.4449,  0.4074, -0.7034, -0.1152,\n",
      "          0.1042, -0.9464, -1.5066,  0.4121,  0.1577,  0.5476,  1.2468, -0.6062,\n",
      "          0.8211, -0.6450,  0.6748,  0.1248, -0.6215],\n",
      "        [ 0.1175,  0.3869,  1.0155,  0.4901,  0.4763, -0.2493,  0.0251, -0.3078,\n",
      "          0.1608, -0.2409,  0.5522,  0.4127,  0.4253,  0.6652,  0.5007,  0.1968,\n",
      "         -0.8093, -0.9517, -0.2374,  0.4420,  0.2178, -1.0286,  0.3246,  1.4862,\n",
      "          0.9875, -0.0183,  1.2295, -0.3054,  0.8612, -0.9049,  0.2900, -0.2052,\n",
      "         -1.3982,  0.3283, -0.2003, -0.7789,  0.0845,  0.6752,  0.7309, -0.8167,\n",
      "         -0.1626,  1.0401,  0.2140, -1.0238,  1.4154,  0.1499, -0.2528,  1.1967,\n",
      "         -1.0389,  0.0131,  1.1025, -0.0077, -0.3343, -0.0045,  0.2536, -1.2108,\n",
      "          0.2154,  0.2241, -0.6292,  0.2614,  0.2612, -0.5607, -1.5256, -1.3169,\n",
      "          0.5294, -0.4138, -0.9402,  0.0830,  0.3199, -0.4408,  0.1537,  0.2856,\n",
      "          0.5180, -0.6955,  1.0143, -1.0798,  0.0640, -0.2057, -1.2363,  0.3803,\n",
      "          0.4062, -0.2826, -0.9767, -0.1639, -0.2495, -0.3109, -0.2404,  0.0188,\n",
      "         -0.2249, -1.2686, -1.6476,  0.0116,  0.7547,  0.8504,  0.6006, -1.3502,\n",
      "          0.8023, -0.4629, -0.3744,  0.7109,  0.4231]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "s1, s2, s3, pred = model(batch, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 100])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SISDRLoss(pred, target):\n",
    "    pred_hat = pred - pred.mean()\n",
    "    target_hat = target - target.mean()\n",
    "    pred_t = torch.inner(pred_hat, target_hat) * target_hat / (target_hat ** 2).sum()\n",
    "    target_t = pred_hat - pred_t\n",
    "    \n",
    "    ret = 20 * torch.log10(torch.norm(pred_t) / (torch.norm(target_t) + 1e-6))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(size=(10, 1, 3)) - torch.ones(size=(10, 1, 3)).mean(dim=(1, 2)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SISDRLoss(torch.ones(size=(10, 1, 3)), torch.ones(size=(10, 1, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled = subsample(batch, batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = linear_proj(subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feed_forward import ConformerFeedForwardLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffl = ConformerFeedForwardLayer(512, 2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_ffl = ffl(lin) + lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ConformerAttentionBlock(512, 8, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_attn = attn(after_ffl) + after_ffl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conv import ConformerConvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ = ConformerConvBlock(in_chanels=512, exp_factor=2, kernel_size=31, padding=15, stride=1, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 31, 512])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1024, 31])\n",
      "torch.Size([3, 512, 31])\n",
      "torch.Size([3, 512, 31])\n",
      "torch.Size([3, 512, 31])\n",
      "torch.Size([3, 512, 31])\n",
      "torch.Size([3, 512, 31])\n",
      "torch.Size([3, 512, 31])\n"
     ]
    }
   ],
   "source": [
    "after_conv = conv_(after_attn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffl_2 = ConformerFeedForwardLayer(512, 2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "shit = (ffl_2(after_conv) + after_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 31, 512])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvSubsample(nn.Module):\n",
    "    def __init__(self, in_chanels, out_chanels):\n",
    "        super().__init__()\n",
    "        self.subsampler = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=out_chanels, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=out_chanels, out_channels=out_chanels, kernel_size=3, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.subsampler(x.unsqueeze(1))\n",
    "        batch_size, channels, subsample_len, subsample_dim = out.shape\n",
    "\n",
    "        out = out.permute(0, 2, 1, 3)\n",
    "        out = out.contiguous().view(batch_size, subsample_len, channels * subsample_dim)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_jopa = ConvSubsample(1, 144)\n",
    "linear = nn.Linear(144 * ((140 - 1)//2 - 1)//2, 144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 140])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.randn(3, 128, 140)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 144 31 34\n",
      "4896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 31, 144])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = testing_jopa(batch)\n",
    "linear(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4896"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "144 * (((140 - 1) // 2 - 1) // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
